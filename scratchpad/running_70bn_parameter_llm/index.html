<!doctype html> <html lang=en > <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start': new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0], j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src= 'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f); })(window,document,'script','dataLayer','GTM-M28VNQP');</script> <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/libs/highlight/styles/github.min.css"> <link href="/css/franklin.css" rel=stylesheet > <link href="/css/vela.css" rel=stylesheet > <script src="/libs/vela/jquery.min.js"></script> <link rel=icon  href="/assets/favicon.png"> <title>Jan's Scratchpad</title> <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-M28VNQP" height=0  width=0  style="display:none;visibility:hidden"></iframe></noscript> <div class="main-nav slideout-menu slideout-menu-left" id=menu > <div class=flex-container > <span class=sidebar-brand > <h3 style='font-size: 25px'>Scratchpad</h3> </span> </div> <nav class=sidebar-nav > <ul class=metismenu  id=metismenu  > <li><a href="/index.html">Home</a> <li><a href="/scratchpad/">Posts</a> <li><a href="/about/">About</a> <li><a href="/tag/">Tags</a> <li><a href="/privacy_policy/">Privacy Policy</a> <li><a href="/cookie_policy/">Cookie Policy</a> </ul> </nav> </div> <main id=panel  class="slidout-panel slideout-panel-left"> <div class="toggle-button hamburger hamburger--spin"> <div class=hamburger-box > <div class=hamburger-inner ></div> </div> </div> <h1 class="page title">Running a 70 Billion-Parameter LLM on Your Laptop?</h1> <hr> <div class=franklin-content ><h1 id=tldr ><a href="#tldr" class=header-anchor >TL;DR</a></h1> <p>You can now run the 70 billion parameter Llama2 language model locally on an M1 Mac in Julia &#40;thanks to llama.cpp 2-bit quantization&#41;</p> <p><div class=franklin-toc ><ol><li><a href="#running_a_70_billion-parameter_llm_on_your_laptop">Running a 70 Billion-Parameter LLM on Your Laptop?</a><li><a href="#llamajl_to_the_rescue">Llama.jl to the rescue&#33;</a><li><a href="#try_the_rocket_model">Try the Rocket model</a><li><a href="#conclusion">Conclusion</a></ol></div> </p> <h2 id=running_a_70_billion-parameter_llm_on_your_laptop ><a href="#running_a_70_billion-parameter_llm_on_your_laptop" class=header-anchor >Running a 70 Billion-Parameter LLM on Your Laptop?</a></h2> <p>It&#39;s a fascinating time in the world of generative AI and large language models. I recently had an experience that seemed almost surreal just a year ago—I ran a 70 billion parameter language model, Llama2, locally on my Mac M1. To put that into perspective, if each parameter were an M&amp;M, we could fill more than 10 Olympic-sized swimming pools&#33;</p> <p>What made this possible is the recent release of Llama.cpp, supporting new 2-bit quantization. This practically compresses the model to use almost 16 times less memory compared to traditional Float32 parameters. It&#39;s an incredible leap in model efficiency and accessibility. It&#39;s not without &quot;price&quot;, but its performance is still impressive.</p> <h2 id=llamajl_to_the_rescue ><a href="#llamajl_to_the_rescue" class=header-anchor >Llama.jl to the rescue&#33;</a></h2> <p>For those eager to try this out, the Julia programming community has made it incredibly easy. Recently, I discovered a nifty package <a href="https://github.com/marcom/Llama.jl">Llama.jl</a>, which wraps the famous llama.cpp, but you don&#39;t have to compile anything&#33; Julia&#39;s Artifact ecosystem streamlines the process.</p> <p>Here’s how you can do it:</p> <pre><code class="julia hljs"><span class=hljs-comment ># Install the package first</span>
<span class=hljs-keyword >using</span> Pkg; Pkg.add(<span class=hljs-string >&quot;https://github.com/marcom/Llama.jl&quot;</span>)

<span class=hljs-keyword >using</span> Llama

<span class=hljs-comment ># Go make a cup of tea while you wait... this is a 20GB download!</span>
url = <span class=hljs-string >&quot;https://huggingface.co/ikawrakow/various-2bit-sota-gguf/resolve/main/llama-v2-70b-2.12bpw.gguf&quot;</span>
model = download_model(url) 

<span class=hljs-comment ># and now we can run the server:</span>
Llama.run_server(; model)
<span class=hljs-comment ># Note: If you get some memory or GPU problems, look at parameter `n_gpu_layers`, which dictates how many layers of your model should go onto your GPU vs CPU</span></code></pre> <p>To try it out either open http://127.0.0.1:10897 in your browser or use PromptingTools.jl like you would with any OpenAI-compatible server:</p> <p>Open a separate Julia session and run:</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> PromptingTools
<span class=hljs-keyword >const</span> PT = PromptingTools

msg = aigenerate(PT.CustomOpenAISchema(), <span class=hljs-string >&quot;What is Julia lang good for?&quot;</span>; api_kwargs=(; url=<span class=hljs-string >&quot;http://127.0.0.1:10897/v1&quot;</span>))</code></pre> <p>It may take a few seconds...</p> <pre><code class="plaintext hljs">[ Info: Tokens: 97 in 19.2 seconds
AIMessage(&quot;Julia is a high-level, high performance dynamic programming language for numerical computing. It provides an ecosystem of open source tools built by the community.&quot;)</code></pre> <p>The above code snippet allows you to download and run a massive 70 billion-parameter model right on your machine with just ~20GB of memory requirement &#40;it&#39;s slightly more than that but not 200GB like the original model&#41;. </p> <h2 id=try_the_rocket_model ><a href="#try_the_rocket_model" class=header-anchor >Try the Rocket model</a></h2> <p>If a 20GB model seems overkill for your needs, try the <a href="https://huggingface.co/pansophic/rocket-3B">Rocket model</a>, which is less than 1GB in size but still boasts 3 billion parameters.</p> <pre><code class="julia hljs"><span class=hljs-comment ># Download the Rocket model and start your server</span>
url = <span class=hljs-string >&quot;https://huggingface.co/ikawrakow/various-2bit-sota-gguf/resolve/main/rocket-3b-2.76bpw.gguf&quot;</span>
model = download_model(url) <span class=hljs-comment ># 1GB download</span>
Llama.run_server(; model)</code></pre> <p>In a separate Julia session, call the model with PromptingTools.jl</p> <pre><code class="julia hljs"><span class=hljs-comment ># In a separate Julia session, call the model with PromptingTools.jl</span>
<span class=hljs-keyword >using</span> PromptingTools
<span class=hljs-keyword >const</span> PT = PromptingTools
<span class=hljs-comment ># PT.register_model!(; name=&quot;llama70b&quot;, schema=PT.CustomOpenAISchema())</span>

msg = aigenerate(PT.CustomOpenAISchema(), <span class=hljs-string >&quot;Say hi!&quot;</span>; api_kwargs=(; url=<span class=hljs-string >&quot;http://127.0.0.1:10897/v1&quot;</span>))
<span class=hljs-comment ># [ Info: Tokens: 75 in 5.9 seconds</span>
<span class=hljs-comment ># AIMessage(&quot;Hello there! I&#x27;m glad you reached out to me. I&#x27;ll do my best to be a helpful AI assistant, so if you have any questions or need assistance with anything, just let me know and I&#x27;d be happy to help. Hi there!&quot;)</span>

msg = aigenerate(PT.CustomOpenAISchema(), <span class=hljs-string >&quot;What is Julia lang good for?&quot;</span>; api_kwargs=(; url=<span class=hljs-string >&quot;http://127.0.0.1:10897/v1&quot;</span>, max_tokens=<span class=hljs-number >2000</span>))
<span class=hljs-comment ># [ Info: Tokens: 137 in 12.6 seconds</span>
<span class=hljs-comment ># AIMessage(&quot;Julia is a high-performance, open-source numerical computing language designed for scientific and engineering applications. It offers fast and efficient computation capabilities with features like multi-threading, automatic array memory optimization, and built-in support for popular libraries such as NumPy, Pandas, and Matplotlib. Julia has gained popularity among data scientists, engineers, and researchers due to its speed, scalability, and ease of use. It is particularly useful when you need to perform complex computations with large datasets or handle high-dimensional arrays efficiently.&quot;)</span></code></pre> <h2 id=conclusion ><a href="#conclusion" class=header-anchor >Conclusion</a></h2> <p>As you can see, the responses are impressive for a 1GB model&#33; It’s truly remarkable how the open-source scene is rapidly advancing, making powerful AI tools accessible to more and more people.</p> <p>This exploration into running large language models locally is just a glimpse into the potential of AI and how it&#39;s being democratized. It&#39;s a testament to the power of open-source software and the continuous innovation in the field of AI and data science. Stay tuned for more amazing developments&#33;</p> <p>PS: Yes, we do need a nicer integration between PromptingTools.jl and Llama.jl. I&#39;m already working on it. Stay tuned&#33;</p> <div class=page-foot > <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> Jan Siml. Last modified: September 15, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. See the <a href="/privacy_policy/">Privacy Policy</a> </div> </div> </main> <script src="/libs/vela/metisMenu.min.js"></script> <script src="/libs/vela/slideout.min.js"></script> <script src="/libs/highlight/highlight.min.js"></script> <script>hljs.highlightAll();hljs.configure({tabReplace: ' '});</script>