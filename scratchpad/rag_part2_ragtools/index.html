<!doctype html> <html lang=en > <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start': new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0], j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src= 'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f); })(window,document,'script','dataLayer','GTM-M28VNQP');</script> <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/libs/highlight/styles/github.min.css"> <link href="/css/franklin.css" rel=stylesheet > <link href="/css/vela.css" rel=stylesheet > <script src="/libs/vela/jquery.min.js"></script> <link rel=icon  href="/assets/favicon.png"> <title>Jan's Scratchpad</title> <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-M28VNQP" height=0  width=0  style="display:none;visibility:hidden"></iframe></noscript> <div class="main-nav slideout-menu slideout-menu-left" id=menu > <div class=flex-container > <span class=sidebar-brand > <h3 style='font-size: 25px'>Scratchpad</h3> </span> </div> <nav class=sidebar-nav > <ul class=metismenu  id=metismenu  > <li><a href="/index.html">Home</a> <li><a href="/scratchpad/">Posts</a> <li><a href="/about/">About</a> <li><a href="/tag/">Tags</a> <li><a href="/privacy_policy/">Privacy Policy</a> <li><a href="/cookie_policy/">Cookie Policy</a> </ul> </nav> </div> <main id=panel  class="slidout-panel slideout-panel-left"> <div class="toggle-button hamburger hamburger--spin"> <div class=hamburger-box > <div class=hamburger-inner ></div> </div> </div> <h1 class="page title">Building a RAG Chatbot over DataFrames.jl Documentation - Easy Mode</h1> <hr> <div class=franklin-content ><h1 id=tldr ><a href="#tldr" class=header-anchor >TL;DR</a></h1> <p>Elevating our RAG chatbot development, this post explores the integration of PromptingTools.jl and RAGTools, showcasing a more efficient approach to building a chatbot using the DataFrames.jl documentation. We&#39;ll also delve into system evaluations.</p> <p><div class=franklin-toc ><ol><li><a href="#effortless_rag_system_building_with_ragtools">Effortless RAG System Building with RAGTools</a><li><a href="#rag_in_two_lines">RAG in Two Lines</a><li><a href="#evaluations_assessing_quality">Evaluations: Assessing Quality</a><ol><li><a href="#generate_qa_pairs">Generate Q&amp;A Pairs</a><li><a href="#explore_a_qa_pair">Explore a Q&amp;A pair</a><li><a href="#judging_a_qa_pair">Judging a Q&amp;A Pair</a></ol><li><a href="#evaluate_the_whole_set">Evaluate the Whole Set</a></ol></div> </p> <p>Last time, we crafted a RAG chatbot from scratch. Today, we&#39;re taking a leap forward with PromptingTools.jl and its experimental sub-module, RAGTools, for a more streamlined experience in Julia. Ready to dive deeper?</p> <h2 id=effortless_rag_system_building_with_ragtools ><a href="#effortless_rag_system_building_with_ragtools" class=header-anchor >Effortless RAG System Building with RAGTools</a></h2> <p>Remember, &quot;RAG&quot; is a cornerstone in Generative AI right now. If you&#39;re new to &quot;RAG&quot;, this <a href="https://towardsdatascience.com/add-your-own-data-to-an-llm-using-retrieval-augmented-generation-rag-b1958bf56a5a">article</a> is a great starting point.</p> <p>Even if you are familiar with RAG, I would strongly recommend watching <a href="https://www.youtube.com/watch?v&#61;TRjq7t2Ms5I&amp;ab_channel&#61;AIEngineer">Jerry Liu&#39;s talk on Building Production-Ready RAG Applications</a>. It&#39;s well spent 20 minutes and will help you understand the further sections in this article.</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> LinearAlgebra, SparseArrays
<span class=hljs-keyword >using</span> PromptingTools
<span class=hljs-keyword >using</span> PromptingTools.Experimental.RAGTools
<span class=hljs-comment >## Note: RAGTools module is still experimental and will change in the future. Ideally, they will be cleaned up and moved to a dedicated package</span>
<span class=hljs-keyword >using</span> JSON3, Serialization, DataFramesMeta
<span class=hljs-keyword >using</span> Statistics: mean
<span class=hljs-keyword >const</span> PT = PromptingTools
<span class=hljs-keyword >const</span> RT = PromptingTools.Experimental.RAGTools</code></pre> <h2 id=rag_in_two_lines ><a href="#rag_in_two_lines" class=header-anchor >RAG in Two Lines</a></h2> <p>Start by grabbing a few text pages from the <a href="https://dataframes.juliadata.org/stable/">DataFrames.jl documentation</a>, saving them as text files in <code>examples/data</code>. Aim for clean content, free of headers and footers. Remember, garbage in, garbage out&#33;</p> <pre><code class="julia hljs">files = [
    joinpath(<span class=hljs-string >&quot;examples&quot;</span>, <span class=hljs-string >&quot;data&quot;</span>, <span class=hljs-string >&quot;database_style_joins.txt&quot;</span>),
    joinpath(<span class=hljs-string >&quot;examples&quot;</span>, <span class=hljs-string >&quot;data&quot;</span>, <span class=hljs-string >&quot;what_is_dataframes.txt&quot;</span>),
]
<span class=hljs-comment ># Build an index of chunks, embed them, and create a lookup index of metadata/tags for each chunk</span>
index = build_index(files; extract_metadata = <span class=hljs-literal >false</span>);</code></pre> <p>Let&#39;s ask a question</p> <pre><code class="julia hljs"><span class=hljs-comment ># Embeds the question, finds the closest chunks in the index, and generates an answer from the closest chunks</span>
answer = airag(index; question = <span class=hljs-string >&quot;I like dplyr, what is the equivalent in Julia?&quot;</span>)</code></pre> <pre><code class="julia hljs">AIMessage(<span class=hljs-string >&quot;The equivalent package in Julia to dplyr in R is DataFramesMeta.jl. It provides convenience functions for data manipulation with syntax similar to dplyr.&quot;</span>)</code></pre>
<p>And there you have it, a RAG system in just two lines&#33;</p>
<p>What does it do?</p>
<ul>
<li><p><code>build_index</code> chunks and embeds the documents, creating a metadata index.</p>
<ul>
<li><p><code>index</code> is the result of this step and it holds your chunks, embeddings, and other metadata&#33; Just show it :&#41;</p>

</ul>

<li><p><code>airag</code> embeds your question, finds relevant chunks, and optionally applies tags or filters to refine the search. It then generates an answer based on the best-matched chunks.</p>
<ul>
<li><p>embed your question</p>

<li><p>find the closest chunks in the index &#40;use parameters <code>top_k</code> and <code>minimum_similarity</code> to tweak the &quot;relevant&quot; chunks&#41;</p>

<li><p>&#91;OPTIONAL&#93; extracts any potential tags/filters from the question and applies them to filter down the potential candidates &#40;use <code>extract_metadata&#61;true</code> in <code>build_index</code>, you can also provide some filters explicitly via <code>tag_filter</code>&#41;</p>

<li><p>&#91;OPTIONAL&#93; re-ranks the candidate chunks &#40;define and provide your own <code>rerank_strategy</code>, eg Cohere ReRank API&#41;</p>

<li><p>build a context from the closest chunks &#40;use <code>chunks_window_margin</code> to tweak if we include preceding and succeeding chunks as well, see <code>?build_context</code> for more details&#41;</p>

</ul>

<li><p>generate an answer from the closest chunks &#40;use <code>return_context&#61;true</code> to see under the hood and debug your application&#41;</p>

</ul>
<p>You should save the index for later to avoid re-embedding / re-extracting the document chunks&#33;</p>
<pre><code class="julia hljs">serialize(<span class=hljs-string >&quot;examples/index.jls&quot;</span>, index)
index = deserialize(<span class=hljs-string >&quot;examples/index.jls&quot;</span>);</code></pre>
<h2 id=evaluations_assessing_quality ><a href="#evaluations_assessing_quality" class=header-anchor >Evaluations: Assessing Quality</a></h2>
<p>To gauge the effectiveness of our system, we need a golden set of quality Q&amp;A pairs. Creating these manually is ideal but can be labor-intensive. Instead, let&#39;s generate them from our index:</p>
<h3 id=generate_qa_pairs ><a href="#generate_qa_pairs" class=header-anchor >Generate Q&amp;A Pairs</a></h3>
<p>Here&#39;s how to create evaluation sets from your <code>index</code> &#40;we need the text chunks and corresponding file paths/sources&#41;.</p>
<pre><code class="julia hljs">evals = build_qa_evals(RT.chunks(index),
    RT.sources(index);
    instructions = <span class=hljs-string >&quot;None.&quot;</span>,
    verbose = <span class=hljs-literal >true</span>);</code></pre>
<pre><code class="julia hljs">[ Info: Q&amp;A Sets built! (cost: $<span class=hljs-number >0.102</span>)</code></pre>
<blockquote>
<p>&#91;&#33;TIP&#93; In practice, you would review each item in this golden evaluation set &#40;and delete any generic/poor questions&#41;. It will determine the future success of your app, so you need to make sure it&#39;s good&#33;</p>
</blockquote>
<p>Save your evaluation sets for later use &#40;and ideally review them manually&#41;.</p>
<pre><code class="julia hljs">JSON3.write(<span class=hljs-string >&quot;examples/evals.json&quot;</span>, evals)
evals = JSON3.read(<span class=hljs-string >&quot;examples/evals.json&quot;</span>, <span class=hljs-built_in >Vector</span>{RT.QAEvalItem});</code></pre>
<h3 id=explore_a_qa_pair ><a href="#explore_a_qa_pair" class=header-anchor >Explore a Q&amp;A pair</a></h3>
<p>Here&#39;s a sample Q&amp;A pair to illustrate the process &#40;it&#39;s not the best quality but gives you the idea&#41;:</p>
<pre><code class="julia hljs">evals[<span class=hljs-number >1</span>]</code></pre>
<pre><code class="julia hljs">QAEvalItem:
 source: examples/data/database_style_joins.txt
 context: Database-Style Joins
Introduction to joins
We often need to combine two or more data sets together to provide a complete picture of the topic we are studying. For example, suppose that we have the following two data sets:

julia&gt; <span class=hljs-keyword >using</span> DataFrames
 question: What is the purpose of joining two or more data sets together?
 answer: The purpose of joining two or more data sets together is to provide a complete picture of the topic being studied.</code></pre>
<h3 id=judging_a_qa_pair ><a href="#judging_a_qa_pair" class=header-anchor >Judging a Q&amp;A Pair</a></h3>
<p>So let&#39;s say we use this Q&amp;A pair to evaluate our system, we plug this Question into <code>airag</code> and get a new answer back. But how do we know it&#39;s good?</p>
<p>We use a &quot;judge model&quot; &#40;like GPT-4&#41; for evaluation &#40;we extract a <code>final_rating</code>&#41;:</p>
<pre><code class="julia hljs"><span class=hljs-comment ># Note: that we used the same question, but generated a different context and answer via `airag`</span>
msg, ctx = airag(index; evals[<span class=hljs-number >1</span>].question, return_context = <span class=hljs-literal >true</span>);
<span class=hljs-comment ># ctx is a RAGContext object that keeps all intermediate states of the RAG pipeline for easy evaluation</span>
judged = aiextract(:RAGJudgeAnswerFromContext;
    ctx.context,
    ctx.question,
    ctx.answer,
    return_type = RT.JudgeAllScores)
judged.content</code></pre>
<pre><code class="julia hljs"><span class=hljs-built_in >Dict</span>{<span class=hljs-built_in >Symbol</span>, <span class=hljs-built_in >Any</span>} with <span class=hljs-number >6</span> entries:
  :final_rating =&gt; <span class=hljs-number >4.8</span>
  :clarity =&gt; <span class=hljs-number >5</span>
  :completeness =&gt; <span class=hljs-number >4</span>
  :relevance =&gt; <span class=hljs-number >5</span>
  :consistency =&gt; <span class=hljs-number >5</span>
  :helpfulness =&gt; <span class=hljs-number >5</span></code></pre>
<p>But <code>final_rating</code> for the generated answer is not the only metric we should watch. We should also judge the quality of the provided context &#40;&quot;retrieval_score&quot;&#41; and a few others.</p>
<p>This generation&#43;evaluation loop and a few common metrics are available in <code>run_qa_evals</code>:</p>
<pre><code class="julia hljs">x = run_qa_evals(evals[<span class=hljs-number >10</span>], ctx;
    parameters_dict = <span class=hljs-built_in >Dict</span>(:top_k =&gt; <span class=hljs-number >3</span>), verbose = <span class=hljs-literal >true</span>, model_judge = <span class=hljs-string >&quot;gpt4t&quot;</span>)</code></pre>
<pre><code class="julia hljs">QAEvalResult:
 source: examples/data/database_style_joins.txt
 context: outerjoin: the output contains rows <span class=hljs-keyword >for</span> values of the key that exist <span class=hljs-keyword >in</span> any of the passed data frames.
semijoin: Like an inner join, but output is restricted to columns from the first (left) argument.
 question: What is the difference between outer join and semi join?
 answer: The purpose of joining two or more data sets together is to combine them <span class=hljs-keyword >in</span> order to provide a complete picture or analysis of a specific topic or dataset. By joining data sets, we can combine information from multiple sources to gain more insights and make more informed decisions.
 retrieval_score: <span class=hljs-number >0.0</span>
 retrieval_rank: <span class=hljs-literal >nothing</span>
 answer_score: <span class=hljs-number >5</span>
 parameters: <span class=hljs-built_in >Dict</span>(:top_k =&gt; <span class=hljs-number >3</span>)</code></pre>
<p>QAEvalResult is a simple struct that holds the evaluation results for a single Q&amp;A pair. It becomes useful when we evaluate a whole set of Q&amp;A pairs &#40;see below&#41;.</p>
<h2 id=evaluate_the_whole_set ><a href="#evaluate_the_whole_set" class=header-anchor >Evaluate the Whole Set</a></h2>
<p>Fortunately, we don&#39;t have to do the evaluations manually one by one.</p>
<p>Let&#39;s run each question &amp; answer through our eval loop in async &#40;we do it only for the first 10 to save time&#41;. See the <code>?airag</code> for which parameters you can tweak, eg, <code>top_k</code></p>
<pre><code class="julia hljs">results = asyncmap(evals[<span class=hljs-number >1</span>:<span class=hljs-number >10</span>]) <span class=hljs-keyword >do</span> qa_item
    <span class=hljs-comment ># Generate an answer -- often you want the model_judge to be the highest quality possible, eg, &quot;GPT-4 Turbo&quot; (alias &quot;gpt4t)</span>
    msg, ctx = airag(index; qa_item.question, return_context = <span class=hljs-literal >true</span>,
        top_k = <span class=hljs-number >3</span>, verbose = <span class=hljs-literal >false</span>, model_judge = <span class=hljs-string >&quot;gpt4t&quot;</span>)
    <span class=hljs-comment ># Evaluate the response</span>
    <span class=hljs-comment ># Note: you can log key parameters for easier analysis later</span>
    run_qa_evals(qa_item, ctx; parameters_dict = <span class=hljs-built_in >Dict</span>(:top_k =&gt; <span class=hljs-number >3</span>), verbose = <span class=hljs-literal >false</span>)
<span class=hljs-keyword >end</span>
<span class=hljs-comment >## Note that the &quot;failed&quot; evals can show as &quot;nothing&quot; (failed as in there was some API error or parsing error), so make sure to handle them.</span>
results = filter(x-&gt;!isnothing(x.answer_score), results);</code></pre>
<p>Note: You could also use the vectorized version <code>results &#61; run_qa_evals&#40;evals&#41;</code> to evaluate all items at once and skip the above code block.</p>
<pre><code class="julia hljs"><span class=hljs-comment ># Let&#x27;s take a simple average to calculate our score</span>
<span class=hljs-meta >@info</span> <span class=hljs-string >&quot;RAG Evals: <span class=hljs-subst >$(length(results)</span>) results, Avg. score: <span class=hljs-subst >$(round(mean(x-&gt;x.answer_score, results)</span>;digits=1)), Retrieval score: <span class=hljs-subst >$(<span class=hljs-number >100</span>*round(<span class=hljs-built_in >Int</span>,mean(x-&gt;x.retrieval_score,results)</span>))%&quot;</span></code></pre>
<pre><code class="julia hljs">[ Info: RAG Evals: <span class=hljs-number >10</span> results, Avg. score: <span class=hljs-number >4.6</span>, Retrieval score: <span class=hljs-number >100</span>%</code></pre>
<p>Note: The retrieval score is 100&#37; only because we have two small documents and running on 10 items only. In practice, you would have a much larger document set and a much larger eval set, which would result in a more representative retrieval score.</p>
<p>If you prefer, you can also analyze the results in a DataFrame:</p>
<pre><code class="julia hljs">df = DataFrame(results)</code></pre>
<p>We&#39;re done for today&#33;</p>
<h1 id=where_to_go_from_here ><a href="#where_to_go_from_here" class=header-anchor >Where to Go From Here?</a></h1>
<ul>
<li><p>Review your evaluation golden data set and keep only the good items</p>

<li><p>Experiment with the chunk sizes &#40;<code>max_length</code> in <code>build_index</code>&#41;</p>

<li><p>Explore using metadata/key filters &#40;<code>extract_metadata&#61;true</code> in <code>build_index</code>&#41;</p>

<li><p>Add filtering for semantic similarity &#40;embedding distance&#41; to make sure we don&#39;t pick up irrelevant chunks in the context &#40;<code>minimum_similarity</code> in <code>airag</code>&#41;</p>

<li><p>Use multiple indices or a hybrid index &#40;add a simple BM25 lookup from <code>TextAnalysis.jl</code>&#41;</p>

<li><p>Data processing is the most important step - properly parsed and split text could make wonders, so review your current approach</p>

<li><p>Add re-ranking of context &#40;see <code>rerank</code> function, you can use Cohere ReRank API&#41;</p>

<li><p>Improve the question embedding &#40;eg, rephrase it, generate hypothetical answers and use them to find better context&#41;</p>

<li><p>Try different models and providers for different parts of the RAG pipeline</p>

</ul>
<p>... and much more&#33; See some ideas in <a href="https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1">Anyscale RAG tutorial</a></p>
<p>If you want to learn more about helpful patterns and advanced evaluation techniques &#40;eg, how to measure &quot;hallucinations&quot;&#41;, check out this talk by <a href="https://www.youtube.com/watch?v&#61;LzeC1AQ-U5o&amp;t&#61;4s&amp;ab_channel&#61;AIEngineer">Eugene Yan: Building Blocks for LLM Systems &amp; Products</a>.</p>
<div class=page-foot >
    <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> Jan Siml. Last modified: December 24, 2023.
    Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. See the <a href="/privacy_policy/">Privacy Policy</a>
</div>
</div>
  </main> 
  <script src="/libs/vela/metisMenu.min.js"></script>
  <script src="/libs/vela/slideout.min.js"></script>
  
  
    <script src="/libs/highlight/highlight.min.js"></script>
<script>hljs.highlightAll();hljs.configure({tabReplace: '    '});</script>