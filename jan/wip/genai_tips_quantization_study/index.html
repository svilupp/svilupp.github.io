<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/libs/highlight/styles/github.min.css"> <!-- bootstrap@5.3.1 and bootstrap icon@1.10--> <!-- <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/css/bootstrap.min.css" rel=stylesheet  integrity="sha384-4bw+/aepP/YC94hEpVNVgiZdgIC5+VKNBQNGCHeKRQN+PtmoHDEXuppvnDJzQIu9" crossorigin=anonymous >--> <link rel=stylesheet  href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.5/font/bootstrap-icons.css"> <link rel=stylesheet  href="https://cdn.jsdelivr.net/npm/@docsearch/css@3"> <link data-n-head=ssr  rel=stylesheet  href="https://fonts.googleapis.com/css?family=Roboto:100,300,400,500,700,900|Material+Icons"> <link rel=stylesheet  href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha512-z3gLpd7yknf1YoNbCzqRKc4qyor8gaKU1qmn+CShxbuBusANI9QpRohGBreCFkKxLhei6S9CQXFEbbKuqLg0DA==" crossorigin=anonymous  referrerpolicy=no-referrer  /> <link rel=stylesheet  href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha512-NhSC1YmyruXifcj/KFRWoC561YpHpc5Jtzgvbuzx5VozKpWvQ+4nXhPdFgmx8xqexRcpAglTj9sIBWINXa8x5w==" crossorigin=anonymous  referrerpolicy=no-referrer  /> <!-- favicon generated through https://realfavicongenerator.net/--> <script src="https://cdn.tailwindcss.com"></script> <link rel=stylesheet  href="/css/franklin.css"> <link rel=apple-touch-icon  sizes=180x180  href="/assets/icon/apple-touch-icon.png"> <link rel=icon  type="image/png" sizes=32x32  href="/assets/icon/favicon-32x32.png"> <link rel=icon  type="image/png" sizes=16x16  href="/assets/icon/favicon-16x16.png"> <link rel=manifest  href="/assets/icon/site.webmanifest"> <link rel=mask-icon  href="/assets/icon/safari-pinned-tab.svg" color="#5bbad5"> <meta name=msapplication-TileColor  content="#da532c"> <meta name=theme-color  content="#ffffff"> <link rel=stylesheet  href="/_css/custom.css"> <title>Quantization Reduces LLM Performance. Or Does It? A Case Study with Yi 34b and Magicoder 7b</title> <!-- {{ispage /jan/index.html}} {{insert head_tailwind.html}} {{end}} {{ispage /index.html}} {{insert head_tailwind.html}} {{end}} {{ispage /jan/wip.html}} {{insert head_tailwind.html}} {{end}} {{ispage /jan/scratchpad/index.html}} {{insert head_tailwind.html}} {{end}} --> <header class=dark > <nav class="bg-gray-800 fixed w-full z-50"> <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"> <div class="flex items-center justify-between h-16"> <div class="flex items-center flex-1"> <a href="/" class="flex items-center text-white"> <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6 mr-2" fill=none  viewBox="0 0 16 16" stroke=currentColor > <path d="M5 10.5a.5.5 0 0 1 .5-.5h2a.5.5 0 0 1 0 1h-2a.5.5 0 0 1-.5-.5zm0-2a.5.5 0 0 1 .5-.5h5a.5.5 0 0 1 0 1h-5a.5.5 0 0 1-.5-.5zm0-2a.5.5 0 0 1 .5-.5h5a.5.5 0 0 1 0 1h-5a.5.5 0 0 1-.5-.5zm0-2a.5.5 0 0 1 .5-.5h5a.5.5 0 0 1 0 1h-5a.5.5 0 0 1-.5-.5z"/> <path d="M3 0h10a2 2 0 0 1 2 2v12a2 2 0 0 1-2 2H3a2 2 0 0 1-2-2v-1h1v1a1 1 0 0 0 1 1h10a1 1 0 0 0 1-1V2a1 1 0 0 0-1-1H3a1 1 0 0 0-1 1v1H1V2a2 2 0 0 1 2-2z"/> <path d="M1 5v-.5a.5.5 0 0 1 1 0V5h.5a.5.5 0 0 1 0 1h-2a.5.5 0 0 1 0-1H1zm0 3v-.5a.5.5 0 0 1 1 0V8h.5a.5.5 0 0 1 0 1h-2a.5.5 0 0 1 0-1H1zm0 3v-.5a.5.5 0 0 1 1 0v.5h.5a.5.5 0 0 1 0 1h-2a.5.5 0 0 1 0-1H1z"/> </svg> <strong>siml.earth</strong> </a> <div class="hidden md:block ml-6"> <div class="flex items-center space-x-4"> <div class="relative group"> <button class="text-gray-300 hover:bg-gray-700 hover:text-white px-3 py-2 rounded-md text-sm font-medium"> Sites </button> <div class="hidden group-hover:block absolute z-50 mt-2 w-48 rounded-md shadow-lg bg-white ring-1 ring-black ring-opacity-5 before:content-[''] before:absolute before:top-[-10px] before:left-0 before:w-full before:h-[10px]"> <div class=py-1 > <div class="px-4 py-2 text-sm text-gray-700">Choose a Site</div> <a href="/jan/" class="block px-4 py-2 text-sm text-gray-700 hover:bg-gray-100">Jan's Site</a> <a href="/ann/" class="block px-4 py-2 text-sm text-gray-700 hover:bg-gray-100">Ann's Site</a> </div> </div> </div> <div class="border-l border-gray-700 h-6"></div> <span class="text-gray-300 font-bold">Jan's:</span> <a href="/jan/" class="text-gray-300 hover:bg-gray-700 hover:text-white px-3 py-2 rounded-md text-sm">Home</a> <a href="/jan/scratchpad/" class="text-gray-300 hover:bg-gray-700 hover:text-white px-3 py-2 rounded-md text-sm">Posts</a> <a href="/jan/wip/" class="text-gray-300 hover:bg-gray-700 hover:text-white px-3 py-2 rounded-md text-sm">Work in Progress</a> <a href="/jan/about/" class="text-gray-300 hover:bg-gray-700 hover:text-white px-3 py-2 rounded-md text-sm">About</a> <a href="/jan/tags/" class="text-gray-300 hover:bg-gray-700 hover:text-white px-3 py-2 rounded-md text-sm">Tags</a> <a href="/jan/privacy_policy/" class="text-gray-300 hover:bg-gray-700 hover:text-white px-3 py-2 rounded-md text-sm">Privacy Policy</a> <a href="/jan/cookie_policy/" class="text-gray-300 hover:bg-gray-700 hover:text-white px-3 py-2 rounded-md text-sm">Cookie Policy</a> </div> </div> </div> <button type=button  class="md:hidden bg-gray-800 inline-flex items-center justify-center p-2 rounded-md text-gray-400 hover:text-white hover:bg-gray-700 focus:outline-none" aria-controls=mobile-menu  aria-expanded=false > <span class=sr-only >Open main menu</span> <svg class="h-6 w-6" fill=none  viewBox="0 0 24 24" stroke=currentColor > <path stroke-linecap=round  stroke-linejoin=round  stroke-width=2  d="M4 6h16M4 12h16M4 18h16" /> </svg> </button> </div> </div> <div class="md:hidden hidden" id=mobile-menu > <div class="px-2 pt-2 pb-3 space-y-1 sm:px-3"> <a href="/jan/" class="text-gray-300 hover:bg-gray-700 hover:text-white block px-3 py-2 rounded-md text-base font-medium">Jan's Site</a> <a href="/ann/" class="text-gray-300 hover:bg-gray-700 hover:text-white block px-3 py-2 rounded-md text-base font-medium">Ann's Site</a> <div class="border-t border-gray-700 my-2"></div> <span class="text-gray-300 font-bold px-3">Jan's:</span> <a href="/jan/" class="text-gray-300 hover:bg-gray-700 hover:text-white block px-3 py-2 rounded-md text-base">Home</a> <a href="/jan/scratchpad/" class="text-gray-300 hover:bg-gray-700 hover:text-white block px-3 py-2 rounded-md text-base">Posts</a> <a href="/jan/wip/" class="text-gray-300 hover:bg-gray-700 hover:text-white block px-3 py-2 rounded-md text-base">Work in Progress</a> <a href="/jan/about/" class="text-gray-300 hover:bg-gray-700 hover:text-white block px-3 py-2 rounded-md text-base">About</a> <a href="/jan/tags/" class="text-gray-300 hover:bg-gray-700 hover:text-white block px-3 py-2 rounded-md text-base">Tags</a> <a href="/jan/privacy_policy/" class="text-gray-300 hover:bg-gray-700 hover:text-white block px-3 py-2 rounded-md text-base">Privacy Policy</a> <a href="/jan/cookie_policy/" class="text-gray-300 hover:bg-gray-700 hover:text-white block px-3 py-2 rounded-md text-base">Cookie Policy</a> </div> </div> </nav> </header> <div class="container px-3 mx-auto pt-20"> <div class=franklin-content ><h1 id=tldr ><a href="#tldr" class=header-anchor >TL;DR</a></h1> <p>A quick preview of my journey in developing a proof of concept for a personalized LLM-powered assistant, aiming to streamline daily productivity tasks. You can do the same&#33;</p> <p><div class=franklin-toc ><ol><li><a href="#tldr">TL;DR</a><ol><li><a href="#introduction_to_model_quantization">Introduction to Model Quantization</a><li><a href="#the_experiment">The Experiment</a><li><a href="#selecting_the_right_quantization">Selecting the Right Quantization</a><li><a href="#the_influence_of_model_size">The Influence of Model Size</a><li><a href="#findings_for_yi34b">Findings for yi34b</a><li><a href="#the_importance_of_temperature">The Importance of Temperature</a><li><a href="#conclusion_and_recommendations">Conclusion and Recommendations</a></ol></ol></div> </p> <p>TODO: add pictures <img src="/assets/genai_tips_quantization_study/yi/model-prompt-comparison-temp0.7.png" alt="Yi34b comparison default temperature" /></p> <p>The pursuit of efficiency in machine learning is a complex balancing act, particularly when it involves model quantization. Our latest study probes the depth of this balance, aiming to understand the trade-offs of using quantized models, the criteria for selecting the right quantization, and the influence of temperature settings on performance.</p> <h2 id=introduction_to_model_quantization ><a href="#introduction_to_model_quantization" class=header-anchor >Introduction to Model Quantization</a></h2> <p>In Julia, practitioners are well-acquainted with different &quot;sizes&quot; of floating-point numbers—Float16, Float32, and Float64—and their respective memory footprints. Just as one can use <code>sizeof&#40;1.0&#41;</code>, which yields 8 bytes for a Float64 in Julia, the concept of model quantization in machine learning involves similar considerations for precision and size. With tools like <a href="https://ollama.ai/">Ollama</a>, selecting the appropriate quantization—be it the more compact Q4<em>0 or the slightly larger and perhaps more capable Q4</em>K_M—mirrors the decision-making process Julians employ when choosing between Float16/32/64 for computational tasks.</p> <p>Model quantization is a process that can significantly reduce the computational cost of running machine learning models. By compressing the model size, it enables faster processing times and lower memory usage, which is crucial for deploying AI in resource-constrained environments. However, the key question remains: What do we sacrifice in terms of performance when we opt for a quantized model?</p> <p>Quantization in machine learning compresses model sizes to enhance efficiency. Among the varieties available, such as AWQ, GPTQ, and EXL2, we focus on those accessible via <a href="https://ollama.ai/">Ollama</a>:</p> <ul> <li><p><strong>Q2_K</strong>: Advanced 2-bit quantization, smallest size &#40;15GB&#41;</p> <li><p><strong>Q4_0</strong>: Basic 4-bit quantization, balances model size &#40;19GB&#41; and performance. Default in Ollama if you run <code>ollama pull</code>.</p> <li><p><strong>Q4<em>K</em>S</strong>/<strong>Q4<em>K</em>M</strong>: Advanced 4-bit quantizations with knowledge distillation, slightly larger &#40;20GB and 21GB respectively&#41; with potential performance benefits.</p> <li><p><strong>FP16</strong>: Less compressed, using 16-bit floating points, largest size &#40;69GB&#41; but often better accuracy.</p> </ul> <p>Each quantization type from Ollama has its trade-offs between computational requirements and performance, with actual runtime memory exceeding file sizes.</p> <h2 id=the_experiment ><a href="#the_experiment" class=header-anchor >The Experiment</a></h2> <p>In our experiment, we scrutinized the performance of two models, yi34b and magicoder7b, which were evaluated using a comprehensive set of benchmarks designed for Julia code generation. With an extensive dataset of over 700 samples per model, our findings offer a robust view of how quantization affects model performance.</p> <ul> <li><p><strong>Benchmark</strong>: Julia LLM Leaderboard &#40;v0.2.0&#41; with 14 test cases and 5 prompts.</p> <li><p><strong>Hardware</strong>: 4x NVIDIA RTX 4090, courtesy of 01.ai.</p> <li><p><strong>Software Backend</strong>: Ollama v0.1.22 for model deployment.</p> <li><p><strong>Software Frontend</strong>: PromptingTools.jl v0.10 for prompt management.</p> <li><p><strong>Scoring</strong>: Range from 0-100; above 25 if it&#39;s parseable, above 50 if it executes some examples, above 75 if it passes some unit tests.</p> <li><p><strong>Volume</strong>: 700 samples tested per model &#40;&#61; 10 x 5 x 14&#41;</p> </ul> <h2 id=selecting_the_right_quantization ><a href="#selecting_the_right_quantization" class=header-anchor >Selecting the Right Quantization</a></h2> <p>The choice of quantization is not to be made lightly. Tools like Ollama default to a Q4_0 version, but our benchmarks suggest that such defaults may not be optimal. This research underscores the importance of making an informed decision when it comes to selecting a quantization level, as it can have a noticeable impact on the model&#39;s capabilities.</p> <h2 id=the_influence_of_model_size ><a href="#the_influence_of_model_size" class=header-anchor >The Influence of Model Size</a></h2> <p>It is commonly understood in the field that the smaller the model, the more pronounced the effects of quantization. This emphasizes the importance of selecting the largest feasible model within operational constraints. When possible, &quot;<em>K</em>M&quot; quantizations are preferred due to their superior performance in our tests.</p> <h2 id=findings_for_yi34b ><a href="#findings_for_yi34b" class=header-anchor >Findings for yi34b</a></h2> <p>Our findings revealed several insights:</p> <ul> <li><p>The Q4<em>K</em>M quantization not only outperformed its counterparts but did so across a range of different prompts, including the more complex &quot;chain-of-thought&quot; templates.</p> <li><p>Surprisingly, FP16, which is not quantized, did not lead the pack and was surpassed by several quantized models, including Q2_K.</p> <li><p>All scores hovered near the 50-point mark, indicating a competency in parsing and running but also pointing to nuanced methodological errors when handling specific inputs.</p> </ul> <h2 id=the_importance_of_temperature ><a href="#the_importance_of_temperature" class=header-anchor >The Importance of Temperature</a></h2> <p>Temperature</p> <p>settings in machine learning models affect the randomness of predictions. A lower temperature results in less random, more confident predictions, while a higher temperature makes the model more uncertain and diverse in its output.</p> <p>In our tests, we experimented with different temperature settings &#40;0.3 and 0.5&#41; for the winning quantization to gauge its effect on performance. Our data showed that altering the temperature did indeed have an effect, although not as drastic as one might expect. It did, however, lead to slight improvements in scores, which could be significant in certain applications.</p> <h2 id=conclusion_and_recommendations ><a href="#conclusion_and_recommendations" class=header-anchor >Conclusion and Recommendations</a></h2> <p>The study&#39;s findings provide clear guidance for practitioners in the field of Generative AI:</p> <ul> <li><p>Quantization does have an effect on performance, with some quantizations leading to better outcomes than others.</p> <li><p>When using automatic tools for model quantization, it is crucial not to settle for default options without consideration. Our benchmark demonstrates that the difference between a default quantization like Q4<em>0 and a more performant one like Q4</em>K_M can be significant.</p> <li><p>For smaller models, the choice of quantization is even more critical. The impact on performance is more pronounced, and thus, careful selection is paramount.</p> <li><p>Where possible, opt for the largest model that is feasible for your computational budget, and prioritize &quot;<em>K</em>M&quot; quantization to maximize performance.</p> </ul> <p>This research adds to the growing body of knowledge on how model quantization affects AI performance, particularly in the context of code generation. As AI continues to integrate more deeply into various fields, understanding these nuances becomes essential. We encourage AI practitioners to consider these findings in their work, selecting quantizations thoughtfully and monitoring the temperature settings to fine-tune their models&#39; performance.</p> <hr /> <p>The insights from this study are not just academic; they are practical considerations that can have a real-world impact on the efficiency and effectiveness of AI applications. As the field progresses, these considerations will only become more crucial.</p> <div class=page-foot > <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> Jan Siml. Last modified: December 11, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia asdasdas programming language</a>. </div> </div></div> <footer class="container mx-auto px-4 py-4 border-t"> <p class="text-right mb-3"> <a href="#" class="text-gray-600 hover:text-gray-900 transition-colors">Back to top</a> </p> <div class="flex flex-col items-center justify-center space-y-3"> <div class=footer-icons > <ul class="flex items-center space-x-4"> <li class="text-gray-700 font-medium">Follow: <li> <a href="https://github.com/svilupp" class="flex items-center space-x-2 text-gray-600 hover:text-gray-900 transition-colors" rel="nofollow noopener noreferrer"> <svg xmlns="http://www.w3.org/2000/svg" width=20  height=20  fill=currentColor  class="bi bi-github" viewBox="0 0 16 16"> <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z"/> </svg> <span>GitHub</span> </a> </ul> </div> <p class="text-sm text-gray-600"> &copy; 2024 · <a href="#" class="hover:text-gray-900 transition-colors">Privacy</a> · <a href="#" class="hover:text-gray-900 transition-colors">Terms</a> </p> </div> </footer> <script src="/libs/highlight/highlight.min.js"></script> <script>hljs.highlightAll();hljs.configure({tabReplace: ' '});</script> <script src="https://cdn.jsdelivr.net/npm/@docsearch/js@3"></script> <script> // Mobile menu toggle const mobileMenuButton = document.querySelector('[aria-controls="mobile-menu"]'); const mobileMenu = document.getElementById('mobile-menu'); mobileMenuButton.addEventListener('click', () => { const expanded = mobileMenuButton.getAttribute('aria-expanded') === 'true'; mobileMenuButton.setAttribute('aria-expanded', !expanded); mobileMenu.classList.toggle('hidden'); }); </script>