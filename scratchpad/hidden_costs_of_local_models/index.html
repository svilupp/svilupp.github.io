<!doctype html> <html lang=en > <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start': new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0], j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src= 'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f); })(window,document,'script','dataLayer','GTM-M28VNQP');</script> <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/libs/highlight/styles/github.min.css"> <link href="/css/franklin.css" rel=stylesheet > <link href="/css/vela.css" rel=stylesheet > <script src="/libs/vela/jquery.min.js"></script> <link rel=icon  href="/assets/favicon.png"> <title>Jan's Scratchpad</title> <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-M28VNQP" height=0  width=0  style="display:none;visibility:hidden"></iframe></noscript> <div class="main-nav slideout-menu slideout-menu-left" id=menu > <div class=flex-container > <span class=sidebar-brand > <h3 style='font-size: 25px'>Scratchpad</h3> </span> </div> <nav class=sidebar-nav > <ul class=metismenu  id=metismenu  > <li><a href="/index.html">Home</a> <li><a href="/scratchpad/">Posts</a> <li><a href="/about/">About</a> <li><a href="/tag/">Tags</a> <li><a href="/privacy_policy/">Privacy Policy</a> <li><a href="/cookie_policy/">Cookie Policy</a> </ul> </nav> </div> <main id=panel  class="slidout-panel slideout-panel-left"> <div class="toggle-button hamburger hamburger--spin"> <div class=hamburger-box > <div class=hamburger-inner ></div> </div> </div> <h1 class="page title">The Hidden Cost of Locally-Hosted Models: A Case Study</h1> <hr> <div class=franklin-content ><h1 id=tldr ><a href="#tldr" class=header-anchor >TL;DR</a></h1> <div class=franklin-toc ><ol><li><a href="#would_you_pay_a_dollar_to_buy_3_extra_days_this_year">Would You Pay a Dollar to Buy 3 Extra Days This Year?</a><li><a href="#appreciating_the_open-source_ai_ecosystem">Appreciating the Open-Source AI Ecosystem</a><li><a href="#the_hidden_costs_of_local_hosting">The Hidden Costs of Local Hosting</a><ol><li><a href="#case_study_daily_coding_assistance">Case Study: Daily Coding Assistance</a></ol><li><a href="#why_choose_cloud_providers">Why Choose Cloud Providers?</a><li><a href="#how_to_start">How to Start?</a><ol><li><a href="#example_usage">Example Usage</a></ol><li><a href="#in_conclusion">In Conclusion</a></ol></div> <h2 id=would_you_pay_a_dollar_to_buy_3_extra_days_this_year ><a href="#would_you_pay_a_dollar_to_buy_3_extra_days_this_year" class=header-anchor >Would You Pay a Dollar to Buy 3 Extra Days This Year?</a></h2> <p>Imagine you could buy time. Not in a metaphorical sense, but literally reclaim hours of your life lost to waiting. For those of us using locally-hosted models for ad-hoc productivity tasks like coding assistance, this isn&#39;t just a daydream—it&#39;s a decision we face every day.</p> <h2 id=appreciating_the_open-source_ai_ecosystem ><a href="#appreciating_the_open-source_ai_ecosystem" class=header-anchor >Appreciating the Open-Source AI Ecosystem</a></h2> <p>First, let&#39;s give credit where it&#39;s due. The thriving open-source ecosystem in generative AI deserves a massive shoutout. Organizations like Meta and Mistral have opened up their models, and platforms like Ollama and Llama.cpp have made these tools accessible for local use. This democratization of technology is nothing short of revolutionary. However, it&#39;s crucial to discuss the true cost of operating these technologies locally &#40;by individuals, for ad-hoc tasks&#41;.</p> <h2 id=the_hidden_costs_of_local_hosting ><a href="#the_hidden_costs_of_local_hosting" class=header-anchor >The Hidden Costs of Local Hosting</a></h2> <p>While the price tag on locally-hosted models might read &quot;free,&quot; the reality is anything but. These models often underperform compared to their cloud-hosted counterparts &#40;GPU-poor&#41; or make you wait longer—sometimes both. For example, using a locally-hosted model like Mixtral on Ollama, you might wait 20 seconds for a response that a commercial provider like Groq, Together, etc. could deliver in less than a second.</p> <h3 id=case_study_daily_coding_assistance ><a href="#case_study_daily_coding_assistance" class=header-anchor >Case Study: Daily Coding Assistance</a></h3> <p>Let&#39;s break it down with a simple case study. Assume you&#39;re a developer making three LLM calls per hour during a three-hour coding session, each day for 250 days a year. That&#39;s 2250 LLM calls.</p> <p>With Ollama, a 20-second wait per call accumulates to over 12 hours spent just waiting annually. </p> <p>In contrast, using Groq&#39;s API, even with an extremely conservative 3-second wait &#40;Llama 3 70b, which is GPT-4 level model&#41;, you&#39;d spend less than 2 hours waiting over the same period.</p> <p>The difference? <strong>More than 10 hours</strong> saved—or, put another way, over 3 extra days of productive coding time each year. </p> <p>And the cost of this extra time? Right now - FREE&#33; Assuming the announced pricing, <strong>about &#36;1.5</strong>.</p> <p>Moreover, with Groq, we assumed using GPT-4 level model&#33; So you would likely benefit even more from MUCH better answers&#33;</p> <h2 id=why_choose_cloud_providers ><a href="#why_choose_cloud_providers" class=header-anchor >Why Choose Cloud Providers?</a></h2> <p>Given <strong>you have roughly 4,000 weeks on this earth</strong>, spending any of them waiting on your GPU seems like a poor use of time. In a way, time is the scarcest resource yet you throw it away to save fractions of cents.</p> <p>Furthermore, you might lose out on innovations. Cloud providers continually upgrade their services with faster and more powerful models without requiring any effort on your part. Meanwhile, changing your local setup is a significant investment and it has its limits &#40;VRAM...&#41;.</p> <h2 id=how_to_start ><a href="#how_to_start" class=header-anchor >How to Start?</a></h2> <p>Switching is simple:</p> <ol> <li><p>Sign up for the <a href="https://console.groq.com/keys">Groq API</a>.</p> <li><p>Set up your environment variable <code>GROQ_API_KEY</code>.</p> <li><p>Use PromptingTools.jl with a Groq-hosted Llama3 70b, which I aliased with &quot;gl70&quot; &#40;Groq Llama 70&#41;. This alias helps save time even when typing&#33;</p> </ol> <h3 id=example_usage ><a href="#example_usage" class=header-anchor >Example Usage</a></h3> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> PromptingTools
<span class=hljs-comment ># Assumes you have set the environment variable GROQ_API_KEY</span>

<span class=hljs-string >ai&quot;In Julia, write a function `clean_names` that cleans up column names of a DataFrame&quot;gl70</span></code></pre> <pre><code class="plaintext hljs">[ Info: Tokens: 411 @ Cost: \$0.0003 in 2.7 seconds
AIMessage(&quot;Here is a Julia function `clean_names` that cleans up column names of a DataFrame:
````julia
using DataFrames
&lt;...continues&gt;
````</code></pre> <p>This simple setup can drastically cut down your waiting time, freeing up days for you to spend on more fulfilling activities or further innovation.</p> <p>If you&#39;re familiar with the <a href="https://github.com/svilupp/PromptingTools.jl">PromptingTools.jl</a> package, you know you can even set up an auto-fixing loop that will execute the generated code, analyze the error for feedback and retry automatically to fix any errors with Monte Carlo Tree Search &#40;see <code>?airetry&#33;</code> for more details&#41;.</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> PromptingTools.Experimental.AgentTools: AIGenerate, run!, AICode
<span class=hljs-keyword >using</span> PromptingTools.Experimental.AgentTools: airetry!, aicodefixer_feedback

result = AIGenerate(
    <span class=hljs-string >&quot;In Julia, write a function `clean_names` that cleans up column names of a DataFrame&quot;</span>;
    model = <span class=hljs-string >&quot;gl70&quot;</span>) |&gt; run!
aicodefixer_feedback
success_func(aicall) = AICode(aicall) |&gt; isvalid
feedback_func(aicall) = aicodefixer_feedback(aicall.conversation).feedback
airetry!(success_func, result, feedback_func; max_retries = <span class=hljs-number >3</span>)</code></pre> <h2 id=in_conclusion ><a href="#in_conclusion" class=header-anchor >In Conclusion</a></h2> <p>While the allure of &quot;free&quot; local hosting is strong, the hidden costs in time can be substantial. By opting for a commercial solution like Groq&#39;s API, not only do you reclaim time lost to waiting, but you also benefit from superior model performance. The investment is minimal compared to the time you buy back—time that could be spent innovating, creating, or just enjoying life. Isn&#39;t that worth considering?</p> <p>If you&#39;re looking to try, do it now while Groq is free&#33;&#33;</p> <div class=page-foot > <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> Jan Siml. Last modified: April 20, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. See the <a href="/privacy_policy/">Privacy Policy</a> </div> </div> </main> <script src="/libs/vela/metisMenu.min.js"></script> <script src="/libs/vela/slideout.min.js"></script> <script src="/libs/highlight/highlight.min.js"></script> <script>hljs.highlightAll();hljs.configure({tabReplace: ' '});</script>