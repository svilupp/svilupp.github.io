<!doctype html> <html lang=en > <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start': new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0], j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src= 'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f); })(window,document,'script','dataLayer','GTM-M28VNQP');</script> <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link href="/css/franklin.css" rel=stylesheet > <link href="/css/vela.css" rel=stylesheet > <script src="/libs/vela/jquery.min.js"></script> <link rel=icon  href="/assets/favicon.png"> <title>Jan's Scratchpad</title> <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-M28VNQP" height=0  width=0  style="display:none;visibility:hidden"></iframe></noscript> <div class="main-nav slideout-menu slideout-menu-left" id=menu > <div class=flex-container > <span class=sidebar-brand > <h3 style='font-size: 25px'>Scratchpad</h3> </span> </div> <nav class=sidebar-nav > <ul class=metismenu  id=metismenu  > <li><a href="/index.html">Home</a> <li><a href="/scratchpad/">Posts</a> <li><a href="/about/">About</a> <li><a href="/tag/">Tags</a> <li><a href="/privacy_policy/">Privacy Policy</a> <li><a href="/cookie_policy/">Cookie Policy</a> </ul> </nav> </div> <main id=panel  class="slidout-panel slideout-panel-left"> <div class="toggle-button hamburger hamburger--spin"> <div class=hamburger-box > <div class=hamburger-inner ></div> </div> </div> <h1 class="page title">Julia LLM Leaderboard: Benchmarking GenAI In Julia Coding</h1> <hr> <div class=franklin-content ><h1 id=tldr ><a href="#tldr" class=header-anchor >TL;DR</a></h1> <p>The <a href="https://github.com/svilupp/Julia-LLM-Leaderboard">Julia LLM Leaderboard</a> is a new benchmarking project that evaluates and compares the Julia code generation capabilities of various Large Language Models, revealing that, unsurprisingly, paid APIs like GPT-4 perform exceptionally well, but the locally-hosted models are quickly closing the gap.</p> <p><div class=franklin-toc ><ol><li><a href="#announcing_the_julia_llm_leaderboard_a_benchmark_for_ai-generated_julia_code">Announcing the Julia LLM Leaderboard: A Benchmark for AI-Generated Julia Code</a><li><a href="#paid_apis">Paid APIs</a><li><a href="#locally-hosted_models">Locally-Hosted Models</a><li><a href="#prompts_prompts_prompts">Prompts, Prompts, Prompts</a><li><a href="#conclusion">Conclusion</a></ol></div> </p> <h2 id=announcing_the_julia_llm_leaderboard_a_benchmark_for_ai-generated_julia_code ><a href="#announcing_the_julia_llm_leaderboard_a_benchmark_for_ai-generated_julia_code" class=header-anchor >Announcing the Julia LLM Leaderboard: A Benchmark for AI-Generated Julia Code</a></h2> <p>We&#39;re excited to announce the launch of the <a href="https://github.com/svilupp/Julia-LLM-Leaderboard">Julia LLM Leaderboard</a>, a comprehensive benchmark project dedicated to evaluating the Julia language generation capabilities of various Large Language Models &#40;LLMs&#41;. This unique repository is designed with a focus on practicality, simplicity, and the Julia community&#39;s needs.</p> <p>The project presents a comparative analysis across multiple AI models, assessing their proficiency in generating syntactically correct Julia code. Our evaluation methodology includes simple and practical criteria like parsing, execution without errors of provided examples and passing unit tests. Each model can score up to 100 points based on these criteria, providing a clear and standardized measure of their capabilities.</p> <h2 id=paid_apis ><a href="#paid_apis" class=header-anchor >Paid APIs</a></h2> <p>Initial findings reveal that paid APIs like GPT-4 and the MistralAI models show impressive performance, with &quot;GPT-4-Turbo-1106&quot; consistently ranking among the highest. </p> <p><img src="/assets/leaderboard_announcement/model-prompt-comparison-paid.png" alt="Performance of Paid APIs across different prompts" /></p> <p>However, if you need a quick response and high-quality outputs, your best choice is &quot;gpt-3.5-turbo-1106&quot; &#40;The &quot;1106&quot; version is important&#33; The default GPT 3.5 Turbo ranks much lower&#41;</p> <p>For more plots and a table summary, visit <a href="https://svilupp.github.io/Julia-LLM-Leaderboard/dev/examples/summarize_results_paid/">Results for Paid APIs</a>.</p> <h2 id=locally-hosted_models ><a href="#locally-hosted_models" class=header-anchor >Locally-Hosted Models</a></h2> <p>Open-source models, though not as robust as the best-paid APIs, are rapidly catching up, with some like Magicoder, Phind CodeLlama, and DeepSeek showing notable results. My personal pick would be &quot;magicoder:7b-s-cl-q6_K&quot; served via <a href=Ollama.ai >Ollama.ai</a>, because it has 7 billion parameters, so it&#39;s quite fast and the performance is solid.</p> <p><img src="/assets/leaderboard_announcement/model-comparison-oss.png" alt="Performance of Locally-Hosted Models" /></p> <p>See more detail <a href="https://siml.earth/Julia-LLM-Leaderboard/dev/examples/summarize_results_oss/">here</a>.</p> <h2 id=prompts_prompts_prompts ><a href="#prompts_prompts_prompts" class=header-anchor >Prompts, Prompts, Prompts</a></h2> <p>Moreover, the benchmark addresses the effectiveness of different prompting strategies. It turns out that even simple prompts can be quite effective, and larger prompts may sometimes confuse smaller models.</p> <p>We used prompting templates available in PromptingTools.jl 0.6.0., except for &quot;AsIs&quot;, which represented the raw task without any mention of Julia language &#40;to see if the LLMs can infer it from the context&#41;.</p> <table><tr><th align=right >Prompt Template<th align=right >Elapsed &#40;s, average&#41;<th align=right >Elapsed &#40;s, median&#41;<th align=right >Avg. Score &#40;Max 100 pts&#41;<th align=right >Median Score &#40;Max 100 pts&#41;<tr><td align=right >InJulia<td align=right >16.7<td align=right >11.7<td align=right >50.6<td align=right >50.0<tr><td align=right >JuliaExpertAsk<td align=right >11.8<td align=right >7.8<td align=right >47.6<td align=right >50.0<tr><td align=right >JuliaRecapTask<td align=right >20.9<td align=right >15.9<td align=right >45.6<td align=right >50.0<tr><td align=right >JuliaExpertCoTTask<td align=right >19.7<td align=right >14.9<td align=right >43.9<td align=right >50.0<tr><td align=right >JuliaRecapCoTTask<td align=right >19.7<td align=right >15.2<td align=right >42.5<td align=right >50.0<tr><td align=right >AsIs<td align=right >36.3<td align=right >11.2<td align=right >13.0<td align=right >0.0</table> <p>Main takeaways:</p> <ul> <li><p>Always make sure to explicitly mention that you want Julia code &#40;the case-in-point is the &quot;AsIs&quot; prompt which performed poorly&#41;</p> <li><p>Just appending &quot;In Julia, ...&quot; can be enough to get a good trade-off of speed and performance</p> <li><p>In many cases, &quot;JuliaExpertAsk&quot; was quite successful. It doesn&#39;t hurt to stroke AI&#39;s ego :&#41;</p> </ul> <h2 id=conclusion ><a href="#conclusion" class=header-anchor >Conclusion</a></h2> <p>These insights are just the tip of the iceberg. The full repository includes detailed documentation of the methodology and results. We invite the Julia community and AI enthusiasts to dive into the <a href="https://github.com/svilupp/Julia-LLM-Leaderboard">Julia LLM Leaderboard</a>, contribute test cases, and explore the fascinating world of AI-generated code.</p> <p>Stay tuned for more in-depth analysis and findings from this project&#33;</p> <div class=page-foot > <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> Jan Siml. Last modified: March 14, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. See the <a href="/privacy_policy/">Privacy Policy</a> </div> </div> </main> <script src="/libs/vela/metisMenu.min.js"></script> <script src="/libs/vela/slideout.min.js"></script>