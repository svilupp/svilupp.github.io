<!doctype html> <html lang=en > <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start': new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0], j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src= 'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f); })(window,document,'script','dataLayer','GTM-M28VNQP');</script> <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link href="/css/franklin.css" rel=stylesheet > <link href="/css/vela.css" rel=stylesheet > <script src="/libs/vela/jquery.min.js"></script> <link rel=icon  href="/assets/favicon.png"> <title>Jan's Scratchpad</title> <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-M28VNQP" height=0  width=0  style="display:none;visibility:hidden"></iframe></noscript> <div class="main-nav slideout-menu slideout-menu-left" id=menu > <div class=flex-container > <span class=sidebar-brand > <h3 style='font-size: 25px'>Scratchpad</h3> </span> </div> <nav class=sidebar-nav > <ul class=metismenu  id=metismenu  > <li><a href="/index.html">Home</a> <li><a href="/scratchpad/">Posts</a> <li><a href="/about/">About</a> <li><a href="/tag/">Tags</a> <li><a href="/privacy_policy/">Privacy Policy</a> <li><a href="/cookie_policy/">Cookie Policy</a> </ul> </nav> </div> <main id=panel  class="slidout-panel slideout-panel-left"> <div class="toggle-button hamburger hamburger--spin"> <div class=hamburger-box > <div class=hamburger-inner ></div> </div> </div> <h1 class="page title">Quantization Reduces LLM Performance. Or Does It? A Case Study with Yi 34b and Magicoder 7b</h1> <hr> <div class=franklin-content ><h1 id=tldr ><a href="#tldr" class=header-anchor >TL;DR</a></h1> <p>A quick preview of my journey in developing a proof of concept for a personalized LLM-powered assistant, aiming to streamline daily productivity tasks. You can do the same&#33;</p> <p><div class=franklin-toc ><ol><li><a href="#introduction_to_model_quantization">Introduction to Model Quantization</a><li><a href="#the_experiment">The Experiment</a><li><a href="#selecting_the_right_quantization">Selecting the Right Quantization</a><li><a href="#the_influence_of_model_size">The Influence of Model Size</a><li><a href="#findings_for_yi34b">Findings for yi34b</a><li><a href="#the_importance_of_temperature">The Importance of Temperature</a><li><a href="#conclusion_and_recommendations">Conclusion and Recommendations</a></ol></div> </p> <p>TODO: add pictures <img src="/assets/genai_tips_quantization_study/yi/model-prompt-comparison-temp0.7.png" alt="Yi34b comparison default temperature" /></p> <p>The pursuit of efficiency in machine learning is a complex balancing act, particularly when it involves model quantization. Our latest study probes the depth of this balance, aiming to understand the trade-offs of using quantized models, the criteria for selecting the right quantization, and the influence of temperature settings on performance.</p> <h2 id=introduction_to_model_quantization ><a href="#introduction_to_model_quantization" class=header-anchor >Introduction to Model Quantization</a></h2> <p>In Julia, practitioners are well-acquainted with different &quot;sizes&quot; of floating-point numbers—Float16, Float32, and Float64—and their respective memory footprints. Just as one can use <code>sizeof&#40;1.0&#41;</code>, which yields 8 bytes for a Float64 in Julia, the concept of model quantization in machine learning involves similar considerations for precision and size. With tools like <a href="https://ollama.ai/">Ollama</a>, selecting the appropriate quantization—be it the more compact Q4<em>0 or the slightly larger and perhaps more capable Q4</em>K_M—mirrors the decision-making process Julians employ when choosing between Float16/32/64 for computational tasks.</p> <p>Model quantization is a process that can significantly reduce the computational cost of running machine learning models. By compressing the model size, it enables faster processing times and lower memory usage, which is crucial for deploying AI in resource-constrained environments. However, the key question remains: What do we sacrifice in terms of performance when we opt for a quantized model?</p> <p>Quantization in machine learning compresses model sizes to enhance efficiency. Among the varieties available, such as AWQ, GPTQ, and EXL2, we focus on those accessible via <a href="https://ollama.ai/">Ollama</a>:</p> <ul> <li><p><strong>Q2_K</strong>: Advanced 2-bit quantization, smallest size &#40;15GB&#41;</p> <li><p><strong>Q4_0</strong>: Basic 4-bit quantization, balances model size &#40;19GB&#41; and performance. Default in Ollama if you run <code>ollama pull</code>.</p> <li><p><strong>Q4<em>K</em>S</strong>/<strong>Q4<em>K</em>M</strong>: Advanced 4-bit quantizations with knowledge distillation, slightly larger &#40;20GB and 21GB respectively&#41; with potential performance benefits.</p> <li><p><strong>FP16</strong>: Less compressed, using 16-bit floating points, largest size &#40;69GB&#41; but often better accuracy.</p> </ul> <p>Each quantization type from Ollama has its trade-offs between computational requirements and performance, with actual runtime memory exceeding file sizes.</p> <h2 id=the_experiment ><a href="#the_experiment" class=header-anchor >The Experiment</a></h2> <p>In our experiment, we scrutinized the performance of two models, yi34b and magicoder7b, which were evaluated using a comprehensive set of benchmarks designed for Julia code generation. With an extensive dataset of over 700 samples per model, our findings offer a robust view of how quantization affects model performance.</p> <ul> <li><p><strong>Benchmark</strong>: Julia LLM Leaderboard &#40;v0.2.0&#41; with 14 test cases and 5 prompts.</p> <li><p><strong>Hardware</strong>: 4x NVIDIA RTX 4090, courtesy of 01.ai.</p> <li><p><strong>Software Backend</strong>: Ollama v0.1.22 for model deployment.</p> <li><p><strong>Software Frontend</strong>: PromptingTools.jl v0.10 for prompt management.</p> <li><p><strong>Scoring</strong>: Range from 0-100; above 25 if it&#39;s parseable, above 50 if it executes some examples, above 75 if it passes some unit tests.</p> <li><p><strong>Volume</strong>: 700 samples tested per model &#40;&#61; 10 x 5 x 14&#41;</p> </ul> <h2 id=selecting_the_right_quantization ><a href="#selecting_the_right_quantization" class=header-anchor >Selecting the Right Quantization</a></h2> <p>The choice of quantization is not to be made lightly. Tools like Ollama default to a Q4_0 version, but our benchmarks suggest that such defaults may not be optimal. This research underscores the importance of making an informed decision when it comes to selecting a quantization level, as it can have a noticeable impact on the model&#39;s capabilities.</p> <h2 id=the_influence_of_model_size ><a href="#the_influence_of_model_size" class=header-anchor >The Influence of Model Size</a></h2> <p>It is commonly understood in the field that the smaller the model, the more pronounced the effects of quantization. This emphasizes the importance of selecting the largest feasible model within operational constraints. When possible, &quot;<em>K</em>M&quot; quantizations are preferred due to their superior performance in our tests.</p> <h2 id=findings_for_yi34b ><a href="#findings_for_yi34b" class=header-anchor >Findings for yi34b</a></h2> <p>Our findings revealed several insights:</p> <ul> <li><p>The Q4<em>K</em>M quantization not only outperformed its counterparts but did so across a range of different prompts, including the more complex &quot;chain-of-thought&quot; templates.</p> <li><p>Surprisingly, FP16, which is not quantized, did not lead the pack and was surpassed by several quantized models, including Q2_K.</p> <li><p>All scores hovered near the 50-point mark, indicating a competency in parsing and running but also pointing to nuanced methodological errors when handling specific inputs.</p> </ul> <h2 id=the_importance_of_temperature ><a href="#the_importance_of_temperature" class=header-anchor >The Importance of Temperature</a></h2> <p>Temperature</p> <p>settings in machine learning models affect the randomness of predictions. A lower temperature results in less random, more confident predictions, while a higher temperature makes the model more uncertain and diverse in its output.</p> <p>In our tests, we experimented with different temperature settings &#40;0.3 and 0.5&#41; for the winning quantization to gauge its effect on performance. Our data showed that altering the temperature did indeed have an effect, although not as drastic as one might expect. It did, however, lead to slight improvements in scores, which could be significant in certain applications.</p> <h2 id=conclusion_and_recommendations ><a href="#conclusion_and_recommendations" class=header-anchor >Conclusion and Recommendations</a></h2> <p>The study&#39;s findings provide clear guidance for practitioners in the field of Generative AI:</p> <ul> <li><p>Quantization does have an effect on performance, with some quantizations leading to better outcomes than others.</p> <li><p>When using automatic tools for model quantization, it is crucial not to settle for default options without consideration. Our benchmark demonstrates that the difference between a default quantization like Q4<em>0 and a more performant one like Q4</em>K_M can be significant.</p> <li><p>For smaller models, the choice of quantization is even more critical. The impact on performance is more pronounced, and thus, careful selection is paramount.</p> <li><p>Where possible, opt for the largest model that is feasible for your computational budget, and prioritize &quot;<em>K</em>M&quot; quantization to maximize performance.</p> </ul> <p>This research adds to the growing body of knowledge on how model quantization affects AI performance, particularly in the context of code generation. As AI continues to integrate more deeply into various fields, understanding these nuances becomes essential. We encourage AI practitioners to consider these findings in their work, selecting quantizations thoughtfully and monitoring the temperature settings to fine-tune their models&#39; performance.</p> <hr /> <p>The insights from this study are not just academic; they are practical considerations that can have a real-world impact on the efficiency and effectiveness of AI applications. As the field progresses, these considerations will only become more crucial.</p> <div class=page-foot > <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> Jan Siml. Last modified: February 13, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. See the <a href="/privacy_policy/">Privacy Policy</a> </div> </div> </main> <script src="/libs/vela/metisMenu.min.js"></script> <script src="/libs/vela/slideout.min.js"></script>