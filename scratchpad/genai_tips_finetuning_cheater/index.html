<!doctype html> <html lang=en > <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start': new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0], j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src= 'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f); })(window,document,'script','dataLayer','GTM-M28VNQP');</script> <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/libs/highlight/styles/github.min.css"> <link href="/css/franklin.css" rel=stylesheet > <link href="/css/vela.css" rel=stylesheet > <script src="/libs/vela/jquery.min.js"></script> <link rel=icon  href="/assets/favicon.png"> <title>Jan's Scratchpad</title> <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-M28VNQP" height=0  width=0  style="display:none;visibility:hidden"></iframe></noscript> <div class="main-nav slideout-menu slideout-menu-left" id=menu > <div class=flex-container > <span class=sidebar-brand > <h3 style='font-size: 25px'>Scratchpad</h3> </span> </div> <nav class=sidebar-nav > <ul class=metismenu  id=metismenu  > <li><a href="/index.html">Home</a> <li><a href="/scratchpad/">Posts</a> <li><a href="/about/">About</a> <li><a href="/tag/">Tags</a> <li><a href="/privacy_policy/">Privacy Policy</a> <li><a href="/cookie_policy/">Cookie Policy</a> </ul> </nav> </div> <main id=panel  class="slidout-panel slideout-panel-left"> <div class="toggle-button hamburger hamburger--spin"> <div class=hamburger-box > <div class=hamburger-inner ></div> </div> </div> <h1 class="page title">A 7 Billion Parameter Model that Beats GPT-4 on Julia Code?</h1> <hr> <div class=franklin-content ><p>@def tags &#61; &#91;&quot;julia&quot;,&quot;generative-AI&quot;,&quot;genAI&quot;,&quot;leaderboard&quot;,&quot;finetuning&quot;&#93;</p> <h1>TL;DR</h1> <p>Fine-tuning AI models for specialized tasks is both cost-effective and straightforward, needing only a few examples and less than a dollar, especially when leveraging tools like Axolotl to simplify the process.</p> <div class=franklin-toc ><ol><li><a href="#cheater-7b">Cheater-7B</a><li><a href="#how_is_it_possible_fine-tuning_cheating">How Is It Possible? Fine-tuning &#43; Cheating&#33;</a><ol><li><a href="#fine-tuning">Fine-tuning</a><li><a href="#beyond_cheating">Beyond Cheating</a></ol><li><a href="#fine-tuning_101">Fine-tuning 101</a><ol><li><a href="#what_is_it">What Is It?</a><li><a href="#why_fine-tuning_should_be_your_new_best_friend">Why Fine-Tuning Should Be Your New Best Friend</a><li><a href="#understanding_the_limits_of_fine-tuning">Understanding the Limits of Fine-Tuning</a><li><a href="#getting_started_with_fine-tuning_easier_than_you_think">Getting Started with Fine-Tuning: Easier Than You Think</a><li><a href="#the_cheater-7b_experiment_fast_affordable_enlightening">The Cheater-7B Experiment: Fast, Affordable, Enlightening</a><li><a href="#getting_started_with_fine-tuning_data">Getting Started with Fine-Tuning Data</a></ol><li><a href="#conclusion">Conclusion</a><li><a href="#resources">Resources</a><li><a href="#extra_questions">Extra Questions</a></ol></div> <h2>Introduction</h2> <p>What if I told you that a David-sized AI model just outsmarted Goliath GPT-4 in Julia code generation? Welcome to the tale of Cheater-7B, our pint-sized hero, whose adventure into fine-tuning showcases the might of focused AI training. The best part? This entire transformation took just 1 hour and cost less than fifty cents.</p> <h2 id=cheater-7b ><a href="#cheater-7b" class=header-anchor >Cheater-7B</a></h2> <p>Cheater-7B is a nimble 7 billion parameter model, fine-tuned to perfection on Julia code. Despite its size, even the quantized version &#40;GGUF Q5&#41;, beats GPT4 in Julia code generation in our <a href="https://github.com/svilupp/Julia-LLM-Leaderboard">leaderboard</a>.</p> <p><img src="/assets/genai_tips_finetuning_cheater/model-comparison.png" alt="Cheater-7b Performance" /></p> <h2 id=how_is_it_possible_fine-tuning_cheating ><a href="#how_is_it_possible_fine-tuning_cheating" class=header-anchor >How Is It Possible? Fine-tuning &#43; Cheating&#33;</a></h2> <h3 id=fine-tuning ><a href="#fine-tuning" class=header-anchor >Fine-tuning</a></h3> <p>Yes, this blog post is a bit of a joke&#33; It is not about the model itself, it’s actually a brief introduction to <strong>fine-tuning</strong>, which allows you to “tune” a smaller model to perform like a big one on a <strong>specific task</strong> &#40;this part is very important.&#41;</p> <p>Fine-tuning Cheater-7B on a select 11 problems demonstrates that you don&#39;t need vast datasets to achieve significant improvements. This little giant not only excelled in familiar territory but also showed promising signs of learning from new, unseen challenges.</p> <h3 id=beyond_cheating ><a href="#beyond_cheating" class=header-anchor >Beyond Cheating</a></h3> <p>Yes, Cheater-7B got a head start by &quot;cheating&quot; on the test&#33; We fine-tuned it on 11/14 test cases in our leaderboard &#40;the one we compare models on&#41; - this happens more often than you think in the real world &#40;often unconsciously&#41;.</p> <p>But the real story here is the power of fine-tuning - because our model turned out to be better than the base model &#40;the model we fine-tuned&#41; in some of the unseen test cases as well&#33; Clearly, it did pick up some Julia knowledge along the way.</p> <h2 id=fine-tuning_101 ><a href="#fine-tuning_101" class=header-anchor >Fine-tuning 101</a></h2> <h3 id=what_is_it ><a href="#what_is_it" class=header-anchor >What Is It?</a></h3> <p>Fine-tuning a model involves adjusting a pre-trained machine learning model&#39;s parameters so it can better perform on a specific task, effectively leveraging the model&#39;s learned knowledge &#40;probability distribution of the next token&#41; and adapting it to new, related challenges with a relatively small dataset.</p> <h3 id=why_fine-tuning_should_be_your_new_best_friend ><a href="#why_fine-tuning_should_be_your_new_best_friend" class=header-anchor >Why Fine-Tuning Should Be Your New Best Friend</a></h3> <p>Fine-tuning stands out for specific tasks &#40;ie, narrow domains&#41; that demand efficiency, and privacy. It&#39;s akin to sharpening your tools to ensure they cut cleaner and faster, all while keeping the costs astonishingly low.</p> <p>Once you build your Generative AI system, sooner or later you will have to route some of the simple requests to smaller fine-tuned models as part of the optimization process. Everyone does that, even the big players.</p> <h3 id=understanding_the_limits_of_fine-tuning ><a href="#understanding_the_limits_of_fine-tuning" class=header-anchor >Understanding the Limits of Fine-Tuning</a></h3> <p>While fine-tuning can transform a general AI model into a specialist, it&#39;s not a silver bullet. This process excels at refining a model&#39;s existing knowledge to perform specific tasks &#40;eg, adjusting the format or style, embedding certain prompts or examples&#41; with greater accuracy or up-weighting/surfacing certain knowledge &#40;eg, Julia&#41; to be used more.</p> <p>However, it does have many limitations. It&#39;s not very effective for tasks that require the model to learn entirely new information or skills from scratch. For such challenges, you might need to incorporate additional learning methods, like Retrieval Augmented Generation &#40;RAG&#41;, to supplement the model&#39;s capabilities. In essence, fine-tuning adjusts the focus of the lens but doesn&#39;t replace the lens altogether.</p> <h3 id=getting_started_with_fine-tuning_easier_than_you_think ><a href="#getting_started_with_fine-tuning_easier_than_you_think" class=header-anchor >Getting Started with Fine-Tuning: Easier Than You Think</a></h3> <p>Diving into fine-tuning is more accessible than ever, thanks to user-friendly tools like Axolotl. This approach not only simplifies the process but also opens the door to a collaborative effort in building specialized, efficient AI models for specific needs.</p> <p>You need very little data to get started - we used just 11 test cases to get started.</p> <p>You can find all the required resources and recipes <a href="https://github.com/svilupp/Julia-LLM-Leaderboard/tree/main/experiments/cheater-7b-finetune">here</a>.</p> <h3 id=the_cheater-7b_experiment_fast_affordable_enlightening ><a href="#the_cheater-7b_experiment_fast_affordable_enlightening" class=header-anchor >The Cheater-7B Experiment: Fast, Affordable, Enlightening</a></h3> <p>The journey of creating Cheater-7B was a lesson in efficiency itself: just 1 hour of processing on a cloud GPU, with an investment that didn&#39;t even hit the half-dollar mark. This experiment underscores the practicality and accessibility of fine-tuning for AI enthusiasts and professionals alike.</p> <h3 id=getting_started_with_fine-tuning_data ><a href="#getting_started_with_fine-tuning_data" class=header-anchor >Getting Started with Fine-Tuning Data</a></h3> <p>Your first step in fine-tuning is to gather examples, specifically AI conversations that align with the skills you&#39;re aiming to enhance &#40;eg, good Julia conversations/exchanges&#41;. To save these conversations for later use, you can employ a helpful function from the PromptingTools package <code>save_conversation</code> &#40;saves a conversation to JSON&#41;.</p> <p>If you&#39;re looking for a communal space to store and share these conversations, consider contributing to an open-source project. Open a pull request at <a href="https://github.com/svilupp/Julia-LLM-Leaderboard/tree/main/julia_conversations">Julia-LLM-Leaderboard&#39;s Julia Conversations</a> to add your valuable data to the collective repository. This folder also shows example code snippets on how to save your conversations from PromptingTools.</p> <p>I hope to write a detailed walkthrough of the process soon, but for now, you can find all the required resources and recipes <a href="https://github.com/svilupp/Julia-LLM-Leaderboard/tree/main/experiments/cheater-7b-finetune">here</a>.</p> <h2 id=conclusion ><a href="#conclusion" class=header-anchor >Conclusion</a></h2> <p>Cheater-7B&#39;s story is more than a quirky anecdote; it&#39;s a compelling illustration of how fine-tuning can unlock the potential of AI models, transforming them into task-specific powerhouses. As we continue to explore and share our experiences, the possibilities for innovation and improvement in AI are boundless. </p> <p>Got a cool idea or breakthrough with your fine-tuning experiments? Share it in the <a href="https://julialang.slack.com/archives/C06G90C697X">generative-ai channel on Julia Slack</a> and inspire the community with your innovation&#33;</p> <h2 id=resources ><a href="#resources" class=header-anchor >Resources</a></h2> <ul> <li><p>Discover Axolotl: <a href="https://github.com/OpenAccess-AI-Collective/axolotl">Axolotl</a></p> <li><p>Explore the Julia LLM Leaderboard: <a href="https://github.com/svilupp/Julia-LLM-Leaderboard">Julia LLM Leaderboard</a></p> <li><p>Resources to Train Your Cheater-7B: <a href="https://github.com/svilupp/Julia-LLM-Leaderboard/tree/main/experiments/cheater-7b-finetune">Cheater-7B experiment</a>.</p> <li><p>Saving Conversations with PromptingTools: <a href="https://github.com/svilupp/Julia-LLM-Leaderboard/tree/main/julia_conversations">Julia Conversations folder</a>.</p> <li><p>Trained Cheater-7B Model: <a href="https://huggingface.co/svilupp/cheater-7b/tree/main">Cheater-7B Model</a></p> </ul> <h2 id=extra_questions ><a href="#extra_questions" class=header-anchor >Extra Questions</a></h2> <ol> <li><p><strong>Is it expensive?</strong> The process of fine-tuning Cheater-7B was surprisingly affordable, costing less than half a dollar. By renting a cloud GPU from Jarvislabs.io and opting for a spot instance outside of peak hours, the entire fine-tuning operation on an RTX A5000 was completed in about an hour for just &#36;0.39.</p> <li><p><strong>How was Cheater-7b trained? Is it difficult?</strong> Training Cheater-7B was streamlined and accessible, thanks to the Axolotl tool. </p> </ol> <p>Axolotl simplifies the fine-tuning process, making it approachable even for those new to machine learning. With just a few commands in the CLI, a configuration YAML file, and the selected dataset, Cheater-7B was fine-tuned efficiently. This ease of use demystifies the process, making advanced AI techniques available to a broader audience. See the example configuration in the Resources section.</p> <ol start=3 > <li><p><strong>Where did you get the data?</strong> The data for fine-tuning Cheater-7B came from the Julia LLM Leaderboard, focusing on solutions that demonstrated excellence and diversity. Specifically, we took the top 50 solutions that scored full points &#40;100 points&#41; for 11 out of the 14 test cases across different prompts. </p> </ol> <p>The associated code is available in the Resources section.</p> <ol start=4 > <li><p><strong>Can I try/use the model?</strong> Yes, of course. Download the LORA adapter or the quantized version from <a href="https://huggingface.co/svilupp/cheater-7b/tree/main">here</a>. I&#39;d recommend using <code>llama.cpp</code> or <code>Llama.jl</code> to run it.</p> <li><p><strong>Did we not just memorize the results?</strong></p> <p>Well, partially&#33; See below the performance of each model &#40;and GPT4 for comparison&#41; on various test cases.</p> <p>We fine-tuned our model on the first 11 test cases. It has never seen any of the last 3 test cases: <code>q_and_a_extractor</code>, <code>pig_latinify</code>, and <code>extra_julia_code</code>. These are the hardest test cases in our leaderboard and you can see that even GPT4 struggles to produce &quot;executable&quot; code &#40;&gt;50 points&#41; for these.</p> <p>The 11 training cases didn&#39;t teach our model much about <code>pig_latinify</code> &#40;requires knowledge of multi-threading and associated libraries&#41; and <code>extract_julia_code</code> &#40;requires large models because there can be multiple nested levels of triple backticks and strings in the inputs, which tips up most models&#41;.</p> <p>However, the performance on <code>q_and_a_extractor</code> has increased significantly compared to both GPT4 and the base model&#33; It&#39;s likely because the model learned how to do Regex operations in Julia and learned to navigate the return types better.</p> <p><img src="/assets/genai_tips_finetuning_cheater/test-case-comparison.png" alt="Test Case Comparison" /></p> </ol> <div class=page-foot > <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> Jan Siml. Last modified: April 25, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. See the <a href="/privacy_policy/">Privacy Policy</a> </div> </div> </main> <script src="/libs/vela/metisMenu.min.js"></script> <script src="/libs/vela/slideout.min.js"></script> <script src="/libs/highlight/highlight.min.js"></script> <script>hljs.highlightAll();hljs.configure({tabReplace: ' '});</script>