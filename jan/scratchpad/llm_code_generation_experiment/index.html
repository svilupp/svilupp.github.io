<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/libs/highlight/styles/github.min.css"> <!-- bootstrap@5.3.1 and bootstrap icon@1.10--> <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/css/bootstrap.min.css" rel=stylesheet  integrity="sha384-4bw+/aepP/YC94hEpVNVgiZdgIC5+VKNBQNGCHeKRQN+PtmoHDEXuppvnDJzQIu9" crossorigin=anonymous > <link rel=stylesheet  href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.5/font/bootstrap-icons.css"> <link rel=stylesheet  href="/css/style.css"> <link rel=stylesheet  href="https://cdn.jsdelivr.net/npm/@docsearch/css@3"> <link data-n-head=ssr  rel=stylesheet  href="https://fonts.googleapis.com/css?family=Roboto:100,300,400,500,700,900|Material+Icons"> <link rel=stylesheet  href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha512-z3gLpd7yknf1YoNbCzqRKc4qyor8gaKU1qmn+CShxbuBusANI9QpRohGBreCFkKxLhei6S9CQXFEbbKuqLg0DA==" crossorigin=anonymous  referrerpolicy=no-referrer  /> <link rel=stylesheet  href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha512-NhSC1YmyruXifcj/KFRWoC561YpHpc5Jtzgvbuzx5VozKpWvQ+4nXhPdFgmx8xqexRcpAglTj9sIBWINXa8x5w==" crossorigin=anonymous  referrerpolicy=no-referrer  /> <!-- favicon generated through https://realfavicongenerator.net/--> <link rel=apple-touch-icon  sizes=180x180  href="/assets/icon/apple-touch-icon.png"> <link rel=icon  type="image/png" sizes=32x32  href="/assets/icon/favicon-32x32.png"> <link rel=icon  type="image/png" sizes=16x16  href="/assets/icon/favicon-16x16.png"> <link rel=manifest  href="/assets/icon/site.webmanifest"> <link rel=mask-icon  href="/assets/icon/safari-pinned-tab.svg" color="#5bbad5"> <meta name=msapplication-TileColor  content="#da532c"> <meta name=theme-color  content="#ffffff"> <link rel=stylesheet  href="/_css/custom.css"> <title>Is there an optimal temperature and top-p for code generation with paid LLM APIs?</title> <header data-bs-theme=dark > <nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark"> <div class=container > <a href="/" class="navbar-brand d-flex align-items-center"> <svg xmlns="http://www.w3.org/2000/svg" width=16  height=16  fill=currentColor  class="bi bi-journal-text me-2" viewBox="0 0 16 16"> <path d="M5 10.5a.5.5 0 0 1 .5-.5h2a.5.5 0 0 1 0 1h-2a.5.5 0 0 1-.5-.5zm0-2a.5.5 0 0 1 .5-.5h5a.5.5 0 0 1 0 1h-5a.5.5 0 0 1-.5-.5zm0-2a.5.5 0 0 1 .5-.5h5a.5.5 0 0 1 0 1h-5a.5.5 0 0 1-.5-.5zm0-2a.5.5 0 0 1 .5-.5h5a.5.5 0 0 1 0 1h-5a.5.5 0 0 1-.5-.5z"/> <path d="M3 0h10a2 2 0 0 1 2 2v12a2 2 0 0 1-2 2H3a2 2 0 0 1-2-2v-1h1v1a1 1 0 0 0 1 1h10a1 1 0 0 0 1-1V2a1 1 0 0 0-1-1H3a1 1 0 0 0-1 1v1H1V2a2 2 0 0 1 2-2z"/> <path d="M1 5v-.5a.5.5 0 0 1 1 0V5h.5a.5.5 0 0 1 0 1h-2a.5.5 0 0 1 0-1H1zm0 3v-.5a.5.5 0 0 1 1 0V8h.5a.5.5 0 0 1 0 1h-2a.5.5 0 0 1 0-1H1zm0 3v-.5a.5.5 0 0 1 1 0v.5h.5a.5.5 0 0 1 0 1h-2a.5.5 0 0 1 0-1H1z"/> </svg> <strong>siml.earth</strong> </a> <button class=navbar-toggler  type=button  data-bs-toggle=collapse  data-bs-target="#navbarCollapse" aria-controls=navbarCollapse  aria-expanded=false  aria-label="Toggle navigation"> <span class=navbar-toggler-icon ></span> </button> <div class="collapse navbar-collapse" id=navbarCollapse > <ul class="navbar-nav me-auto mb-2 mb-md-0"> <li class="nav-item dropdown"> <a class="nav-link dropdown-toggle fw-bold" href="#" id=navbarDropdown  role=button  data-bs-toggle=dropdown  aria-expanded=false > Sites </a> <ul class=dropdown-menu  aria-labelledby=navbarDropdown > <li><h6 class=dropdown-header >Choose a Site</h6> <li><a class=dropdown-item  href="/jan/">Jan's Site</a> <li><a class=dropdown-item  href="/ann/">Ann's Site</a> </ul> <li class=nav-item ><hr class="dropdown-divider mx-2"> <li class=nav-item > <span class="navbar-text px-2 fw-bold d-flex align-items-center">Jan's:</span> <li class=nav-item > <a class=nav-link  aria-current=page  href="/jan/">Home</a> <li class=nav-item > <a class=nav-link  href="/jan/scratchpad/">Posts</a> <li class=nav-item > <a class=nav-link  href="/jan/wip/">Work in Progress</a> <li class=nav-item > <a class=nav-link  href="/jan/about/">About</a> <li class=nav-item > <a class=nav-link  href="/jan/tags/">Tags</a> <li class=nav-item > <a class=nav-link  href="/jan/privacy_policy/">Privacy Policy</a> <li class=nav-item > <a class=nav-link  href="/jan/cookie_policy/">Cookie Policy</a> </ul> </div> </div> </nav> </header> <div class="container py-3 px-3 mx-auto"> <div class=franklin-content ><h1 id=tldr ><a href="#tldr" class=header-anchor >TL;DR</a></h1> <p>After experimenting with various API parameters for OpenAI and MistralAI, I found that tweaking two settings—temperature and top_p—could boost code generation performance. But these AI models are like the weather in London; they change so often that today&#39;s &quot;perfect&quot; settings might be outdated by tomorrow. So, rather than chase the elusive &#39;perfect&#39; setup, it&#39;s wiser to focus on creating robust tests for your AI&#39;s performance. Keep it simple and let the AI do the heavy lifting&#33;</p> <p><div class=franklin-toc ><ol><li><a href="#tldr">TL;DR</a><li><a href="#appendix_winning_hyperparameters_for_each_model">Appendix: &quot;Winning&quot; Hyperparameters for each Model</a><ol><li><a href="#gpt-4-1106-preview">GPT-4-1106-Preview</a><li><a href="#mistral-medium">Mistral-Medium</a><li><a href="#gpt-35-turbo-1106">GPT-3.5-Turbo-1106</a><li><a href="#mistral-small">Mistral-Small</a><li><a href="#gpt-35-turbo">GPT-3.5-Turbo</a><li><a href="#mistral-tiny">Mistral-Tiny</a></ol></ol></div> </p> <p>Last week, the buzz was all about MistralAI&#39;s new API launch, featuring the enigmatic &quot;mistral-medium&quot;—a tier that&#39;s not as widely discussed as the hyped &quot;Mixtral 8x7B&quot; &#40;&quot;mistral-small&quot; on Mistral&#39;s La Plateforme&#41;. Curious, I took my Julia code generation benchmarks for a spin and noticed that &quot;mistral-medium&quot; wasn&#39;t significantly outperforming its smaller sibling.</p> <p>Here&#39;s the catch: such results are only relevant to my mini benchmark and may not hold true across other domains. So, I pondered, could this be due to suboptimal hyperparameters? With that in mind, I decided to tinker with temperature and top_p—parameters that essentially control the creativity and focus of the AI&#39;s responses.</p> <p>I needed a test dataset. Fortunately, there is a <a href="https://github.com/svilupp/Julia-LLM-Leaderboard">Julia-LLM-Leaderboard</a> which has a collection of Julia code generation tasks with a corresponding automated evaluation framework. Each run can score between 0-100 points, where 100 points is the best.</p> <p>My experiment was straightforward. I ran a grid search across 36 combinations of temperature and top_p values, refining the process until I found what seemed like &quot;sweet spots.&quot; &#40;<a href="https://github.com/svilupp/Julia-LLM-Leaderboard/tree/main/experiments/hyperparams-search-paid-apis-v01">detail here</a>&#41;. I did the same for 3 OpenAI and 3 Mistral models.</p> <p>Interestingly, mistral-medium&#39;s performance soared from 54 to 87 points by adjusting to top_p: 0.3 and temperature: 0.9. </p> <p><img src="/assets/llm_code_generation_experiment/mistral-medium-parameter-search-stage2-20231215.png" alt=mistral-medium-first-results  /></p> <p>This has been after c. 200 runs &#40;representing &lt;20&#37; of the available test cases&#41;. I decided to pick these as the new &quot;optimal&quot; parameters and re-run the full benchmark &#40;I did the same for all other models as well&#41;.</p> <p>But here&#39;s the twist—repeating the benchmark revealed no significant change. After a bit of sleuthing, I discovered the API&#39;s model had been updated, rendering my &quot;optimal&quot; parameters outdated.</p> <p>See how the same heatmap looked one day later:</p> <p><img src="/assets/llm_code_generation_experiment/mistral-medium-parameter-search-stage2-20231216.png" alt=mistral-medium-later-results  /></p> <p>&quot;Wait, isn&#39;t it just because you didn&#39;t run enough samples?&quot;</p> <p>While it&#39;s valid to point out the stochastic behavior of these models, with scores potentially fluctuating from one minute to the next, my multi-stage experiment displayed a remarkable consistency in the top-performing parameters &#40;different on each day&#41;. This consistency suggests that, despite the inherent randomness, there seem to be some &#39;optimal&#39; settings that can be identified for specific classes of problems.</p> <p><strong>So, are there optimal parameters?</strong> Yes, but they&#39;re fleeting, tied to the API&#39;s &#43; model&#39;s current version. </p> <p><strong>Is it worth obsessing over them?</strong> For most use cases, probably not. </p> <p><strong>The takeaway?</strong> Focus on a robust evaluation dataset, and let the API handle the rest.</p> <p>Curiosity led to this experiment, and while the pursuit of perfection is alluring, the shifting nature of AI models means that we&#39;re better off embracing adaptability in our everyday use.</p> <p>If you&#39;re interested in the results for all the other models I tested, check out the appendix below.</p> <p>A few observations:</p> <ul> <li><p>You want to think about <code>top_p</code> and <code>temperate</code> together, not in isolation</p> <li><p>Keep their sum around 1.0 &#40;or at least lower than the default 0.7&#43;1.0 &#61; 1.7&#41;</p> </ul> <h1 id=appendix_winning_hyperparameters_for_each_model ><a href="#appendix_winning_hyperparameters_for_each_model" class=header-anchor >Appendix: &quot;Winning&quot; Hyperparameters for each Model</a></h1> <p>Dive into the appendix for a granular view of each model&#39;s performance in our experiments. It&#39;s worth mentioning that I&#39;ve recently enhanced the evaluation parser to more equitably assess smaller OSS models. This adjustment may have caused a slight shift in the results. You might notice a few &quot;high scores&quot; that are supported by a limited number of samples; these are remnants of the previous scoring system and should be interpreted with caution.</p> <p>In other words, don&#39;t take these results as gospel. Instead, use them as a starting point for your own experiments.</p> <h2 id=gpt-4-1106-preview ><a href="#gpt-4-1106-preview" class=header-anchor >GPT-4-1106-Preview</a></h2> <p>The GPT-4-1106-Preview model showed remarkable adaptability in the grid search, with the top three hyperparameter combinations centered around extremes of temperature and top<em>p. Notably, the combination with a low temperature of 0.1 and a high top</em>p of 0.9 yielded the highest score of approximately 87.22. This suggests a preference for highly deterministic output with a wide selection pool, a setting that may be beneficial for generating more creative yet precise code.</p> <p><img src="/assets/llm_code_generation_experiment/gpt-4-1106-preview-parameter-search.png" alt="GPT-4-1106-Preview Heatmap" /></p> <h2 id=mistral-medium ><a href="#mistral-medium" class=header-anchor >Mistral-Medium</a></h2> <p>Mistral-Medium displayed a significant increase in performance when the temperature was set high at 0.9, coupled with a more selective top_p of 0.3, scoring around 82.81. This indicates that a warmer temperature, allowing for more diverse responses, in combination with a moderate selection probability, optimizes performance for this model.</p> <p><img src="/assets/llm_code_generation_experiment/mistral-medium-parameter-search.png" alt="Mistral-Medium Heatmap" /></p> <h2 id=gpt-35-turbo-1106 ><a href="#gpt-35-turbo-1106" class=header-anchor >GPT-3.5-Turbo-1106</a></h2> <p>For GPT-3.5-Turbo-1106, the best results came from a high temperature of 0.9 and a low top_p of 0.1, with a score close to 81.25. This pattern aligns with a tendency towards creative responses but with a narrow choice spectrum, which seems to enhance performance for this particular model.</p> <p><img src="/assets/llm_code_generation_experiment/gpt-3.5-turbo-1106-parameter-search.png" alt="GPT-3.5-Turbo-1106 Heatmap" /></p> <h2 id=mistral-small ><a href="#mistral-small" class=header-anchor >Mistral-Small</a></h2> <p>Note: Due to the evaluation parser improvements, the scores for the mistral-small model have changed slightly. The highest scoring combination with sufficient sample size is still 0.9/0.3 &#40;same as mistral-medium&#41;, the highest value in the heatmap &#40;85.0&#41; does not have sufficient sample size &#40;only 1 run&#41;.</p> <p><img src="/assets/llm_code_generation_experiment/mistral-small-parameter-search.png" alt="Mistral-Small Heatmap" /></p> <h2 id=gpt-35-turbo ><a href="#gpt-35-turbo" class=header-anchor >GPT-3.5-Turbo</a></h2> <p>The GPT-3.5-Turbo favored a temperature of 0.9 and top<em>p set at 0.5 yielding 70.39. However, this score is fairly closed to a more balanced setting of 0.5 for both temperature and top</em>p with medium variability and selection probability, which achieved a score of approximately 68.11.</p> <p><img src="/assets/llm_code_generation_experiment/gpt-3.5-turbo-parameter-search.png" alt="GPT-3.5-Turbo Heatmap" /></p> <h2 id=mistral-tiny ><a href="#mistral-tiny" class=header-anchor >Mistral-Tiny</a></h2> <p>Note: All the re-sampled combinations from Stage 2 drop off this table to performance ~0.5. Ie, no need to keep re-sampling the &quot;top&quot; combinations, they are just a noise/lucky shot.</p> <p><img src="/assets/llm_code_generation_experiment/mistral-tiny-parameter-search.png" alt="Mistral-Tiny Heatmap" /></p> <div class=page-foot > <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> Jan Siml. Last modified: November 25, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div></div> <footer class=container > <p class=float-end ><a href="#">Back to top</a></p> <div class=footer-icons > <ul class=social-icons > <li><strong>Follow:</strong> <li><a href="https://github.com/svilupp" rel="nofollow noopener noreferrer"><svg xmlns="http://www.w3.org/2000/svg" width=16  height=16  fill=currentColor  class="bi bi-github" viewBox="0 0 16 16"> <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z"/> </svg> GitHub</a> </ul> </div> <p class=copyright >&copy; 2023- Company, Inc. · <a href="#">Privacy</a> · <a href="#">Terms</a></p> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/js/bootstrap.bundle.min.js" integrity=sha384-HwwvtgBNo3bZJJLYd8oVXjrBZt8cqVSpeBNS5n7C8IVInixGAoxmnlMuBnhbgrkm  crossorigin=anonymous ></script> <script src="/libs/highlight/highlight.min.js"></script> <script>hljs.highlightAll();hljs.configure({tabReplace: ' '});</script> <script src="https://cdn.jsdelivr.net/npm/@docsearch/js@3"></script>