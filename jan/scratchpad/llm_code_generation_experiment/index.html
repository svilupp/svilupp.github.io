<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/libs/highlight/styles/github.min.css"> <!-- bootstrap@5.3.1 and bootstrap icon@1.10--> <!-- <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/css/bootstrap.min.css" rel=stylesheet  integrity="sha384-4bw+/aepP/YC94hEpVNVgiZdgIC5+VKNBQNGCHeKRQN+PtmoHDEXuppvnDJzQIu9" crossorigin=anonymous >--> <link rel=stylesheet  href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.5/font/bootstrap-icons.css"> <link rel=stylesheet  href="https://cdn.jsdelivr.net/npm/@docsearch/css@3"> <link data-n-head=ssr  rel=stylesheet  href="https://fonts.googleapis.com/css?family=Roboto:100,300,400,500,700,900|Material+Icons"> <link rel=stylesheet  href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha512-z3gLpd7yknf1YoNbCzqRKc4qyor8gaKU1qmn+CShxbuBusANI9QpRohGBreCFkKxLhei6S9CQXFEbbKuqLg0DA==" crossorigin=anonymous  referrerpolicy=no-referrer  /> <link rel=stylesheet  href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha512-NhSC1YmyruXifcj/KFRWoC561YpHpc5Jtzgvbuzx5VozKpWvQ+4nXhPdFgmx8xqexRcpAglTj9sIBWINXa8x5w==" crossorigin=anonymous  referrerpolicy=no-referrer  /> <!-- favicon generated through https://realfavicongenerator.net/--> <script src="https://cdn.tailwindcss.com"></script> <link rel=stylesheet  href="/css/franklin.css"> <link rel=apple-touch-icon  sizes=180x180  href="/assets/icon/apple-touch-icon.png"> <link rel=icon  type="image/png" sizes=32x32  href="/assets/icon/favicon-32x32.png"> <link rel=icon  type="image/png" sizes=16x16  href="/assets/icon/favicon-16x16.png"> <link rel=manifest  href="/assets/icon/site.webmanifest"> <link rel=mask-icon  href="/assets/icon/safari-pinned-tab.svg" color="#5bbad5"> <meta name=msapplication-TileColor  content="#da532c"> <meta name=theme-color  content="#ffffff"> <link rel=stylesheet  href="/_css/custom.css"> <title>Is there an optimal temperature and top-p for code generation with paid LLM APIs?</title> <!-- {{ispage /jan/index.html}} {{insert head_tailwind.html}} {{end}} {{ispage /index.html}} {{insert head_tailwind.html}} {{end}} {{ispage /jan/wip.html}} {{insert head_tailwind.html}} {{end}} {{ispage /jan/scratchpad/index.html}} {{insert head_tailwind.html}} {{end}} --> <header class=dark > <nav class="bg-gray-800 fixed w-full z-50"> <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"> <div class="flex items-center justify-between h-16"> <div class="flex items-center flex-1"> <a href="/" class="flex items-center text-white"> <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6 mr-2" fill=none  viewBox="0 0 16 16" stroke=currentColor > <path d="M5 10.5a.5.5 0 0 1 .5-.5h2a.5.5 0 0 1 0 1h-2a.5.5 0 0 1-.5-.5zm0-2a.5.5 0 0 1 .5-.5h5a.5.5 0 0 1 0 1h-5a.5.5 0 0 1-.5-.5zm0-2a.5.5 0 0 1 .5-.5h5a.5.5 0 0 1 0 1h-5a.5.5 0 0 1-.5-.5zm0-2a.5.5 0 0 1 .5-.5h5a.5.5 0 0 1 0 1h-5a.5.5 0 0 1-.5-.5z"/> <path d="M3 0h10a2 2 0 0 1 2 2v12a2 2 0 0 1-2 2H3a2 2 0 0 1-2-2v-1h1v1a1 1 0 0 0 1 1h10a1 1 0 0 0 1-1V2a1 1 0 0 0-1-1H3a1 1 0 0 0-1 1v1H1V2a2 2 0 0 1 2-2z"/> <path d="M1 5v-.5a.5.5 0 0 1 1 0V5h.5a.5.5 0 0 1 0 1h-2a.5.5 0 0 1 0-1H1zm0 3v-.5a.5.5 0 0 1 1 0V8h.5a.5.5 0 0 1 0 1h-2a.5.5 0 0 1 0-1H1zm0 3v-.5a.5.5 0 0 1 1 0v.5h.5a.5.5 0 0 1 0 1h-2a.5.5 0 0 1 0-1H1z"/> </svg> <strong>siml.earth</strong> </a> <div class="hidden md:block ml-6"> <div class="flex items-center space-x-4"> <div class="relative group"> <button class="text-gray-300 hover:bg-gray-700 hover:text-white px-3 py-2 rounded-md text-sm font-medium"> Sites </button> <div class="hidden group-hover:block absolute z-50 mt-2 w-48 rounded-md shadow-lg bg-white ring-1 ring-black ring-opacity-5 before:content-[''] before:absolute before:top-[-10px] before:left-0 before:w-full before:h-[10px]"> <div class=py-1 > <div class="px-4 py-2 text-sm text-gray-700">Choose a Site</div> <a href="/jan/" class="block px-4 py-2 text-sm text-gray-700 hover:bg-gray-100">Jan's Site</a> <a href="/ann/" class="block px-4 py-2 text-sm text-gray-700 hover:bg-gray-100">Ann's Site</a> </div> </div> </div> <div class="border-l border-gray-700 h-6"></div> <span class="text-gray-300 font-bold">Jan's:</span> <a href="/jan/" class="text-gray-300 hover:bg-gray-700 hover:text-white px-3 py-2 rounded-md text-sm">Home</a> <a href="/jan/scratchpad/" class="text-gray-300 hover:bg-gray-700 hover:text-white px-3 py-2 rounded-md text-sm">Posts</a> <a href="/jan/wip/" class="text-gray-300 hover:bg-gray-700 hover:text-white px-3 py-2 rounded-md text-sm">Work in Progress</a> <a href="/jan/about/" class="text-gray-300 hover:bg-gray-700 hover:text-white px-3 py-2 rounded-md text-sm">About</a> <a href="/jan/tags/" class="text-gray-300 hover:bg-gray-700 hover:text-white px-3 py-2 rounded-md text-sm">Tags</a> <a href="/jan/privacy_policy/" class="text-gray-300 hover:bg-gray-700 hover:text-white px-3 py-2 rounded-md text-sm">Privacy Policy</a> <a href="/jan/cookie_policy/" class="text-gray-300 hover:bg-gray-700 hover:text-white px-3 py-2 rounded-md text-sm">Cookie Policy</a> </div> </div> </div> <button type=button  class="md:hidden bg-gray-800 inline-flex items-center justify-center p-2 rounded-md text-gray-400 hover:text-white hover:bg-gray-700 focus:outline-none" aria-controls=mobile-menu  aria-expanded=false > <span class=sr-only >Open main menu</span> <svg class="h-6 w-6" fill=none  viewBox="0 0 24 24" stroke=currentColor > <path stroke-linecap=round  stroke-linejoin=round  stroke-width=2  d="M4 6h16M4 12h16M4 18h16" /> </svg> </button> </div> </div> <div class="md:hidden hidden" id=mobile-menu > <div class="px-2 pt-2 pb-3 space-y-1 sm:px-3"> <a href="/jan/" class="text-gray-300 hover:bg-gray-700 hover:text-white block px-3 py-2 rounded-md text-base font-medium">Jan's Site</a> <a href="/ann/" class="text-gray-300 hover:bg-gray-700 hover:text-white block px-3 py-2 rounded-md text-base font-medium">Ann's Site</a> <div class="border-t border-gray-700 my-2"></div> <span class="text-gray-300 font-bold px-3">Jan's:</span> <a href="/jan/" class="text-gray-300 hover:bg-gray-700 hover:text-white block px-3 py-2 rounded-md text-base">Home</a> <a href="/jan/scratchpad/" class="text-gray-300 hover:bg-gray-700 hover:text-white block px-3 py-2 rounded-md text-base">Posts</a> <a href="/jan/wip/" class="text-gray-300 hover:bg-gray-700 hover:text-white block px-3 py-2 rounded-md text-base">Work in Progress</a> <a href="/jan/about/" class="text-gray-300 hover:bg-gray-700 hover:text-white block px-3 py-2 rounded-md text-base">About</a> <a href="/jan/tags/" class="text-gray-300 hover:bg-gray-700 hover:text-white block px-3 py-2 rounded-md text-base">Tags</a> <a href="/jan/privacy_policy/" class="text-gray-300 hover:bg-gray-700 hover:text-white block px-3 py-2 rounded-md text-base">Privacy Policy</a> <a href="/jan/cookie_policy/" class="text-gray-300 hover:bg-gray-700 hover:text-white block px-3 py-2 rounded-md text-base">Cookie Policy</a> </div> </div> </nav> </header> <div class="container px-3 mx-auto pt-20"> <div class="max-w-4xl mx-auto"> <h1 class="text-4xl font-bold mb-4 text-center">Is there an optimal temperature and top-p for code generation with paid LLM APIs?</h1> <p class="text-gray-600 mb-8 text-center">18 December 2023</p> </div> <div class=franklin-content ><h1 id=tldr ><a href="#tldr" class=header-anchor >TL;DR</a></h1> <p>After experimenting with various API parameters for OpenAI and MistralAI, I found that tweaking two settings—temperature and top_p—could boost code generation performance. But these AI models are like the weather in London; they change so often that today&#39;s &quot;perfect&quot; settings might be outdated by tomorrow. So, rather than chase the elusive &#39;perfect&#39; setup, it&#39;s wiser to focus on creating robust tests for your AI&#39;s performance. Keep it simple and let the AI do the heavy lifting&#33;</p> <p><div class=franklin-toc ><ol><li><a href="#tldr">TL;DR</a><li><a href="#appendix_winning_hyperparameters_for_each_model">Appendix: &quot;Winning&quot; Hyperparameters for each Model</a><ol><li><a href="#gpt-4-1106-preview">GPT-4-1106-Preview</a><li><a href="#mistral-medium">Mistral-Medium</a><li><a href="#gpt-35-turbo-1106">GPT-3.5-Turbo-1106</a><li><a href="#mistral-small">Mistral-Small</a><li><a href="#gpt-35-turbo">GPT-3.5-Turbo</a><li><a href="#mistral-tiny">Mistral-Tiny</a></ol></ol></div> </p> <p>Last week, the buzz was all about MistralAI&#39;s new API launch, featuring the enigmatic &quot;mistral-medium&quot;—a tier that&#39;s not as widely discussed as the hyped &quot;Mixtral 8x7B&quot; &#40;&quot;mistral-small&quot; on Mistral&#39;s La Plateforme&#41;. Curious, I took my Julia code generation benchmarks for a spin and noticed that &quot;mistral-medium&quot; wasn&#39;t significantly outperforming its smaller sibling.</p> <p>Here&#39;s the catch: such results are only relevant to my mini benchmark and may not hold true across other domains. So, I pondered, could this be due to suboptimal hyperparameters? With that in mind, I decided to tinker with temperature and top_p—parameters that essentially control the creativity and focus of the AI&#39;s responses.</p> <p>I needed a test dataset. Fortunately, there is a <a href="https://github.com/svilupp/Julia-LLM-Leaderboard">Julia-LLM-Leaderboard</a> which has a collection of Julia code generation tasks with a corresponding automated evaluation framework. Each run can score between 0-100 points, where 100 points is the best.</p> <p>My experiment was straightforward. I ran a grid search across 36 combinations of temperature and top_p values, refining the process until I found what seemed like &quot;sweet spots.&quot; &#40;<a href="https://github.com/svilupp/Julia-LLM-Leaderboard/tree/main/experiments/hyperparams-search-paid-apis-v01">detail here</a>&#41;. I did the same for 3 OpenAI and 3 Mistral models.</p> <p>Interestingly, mistral-medium&#39;s performance soared from 54 to 87 points by adjusting to top_p: 0.3 and temperature: 0.9. </p> <p><img src="/assets/llm_code_generation_experiment/mistral-medium-parameter-search-stage2-20231215.png" alt=mistral-medium-first-results  /></p> <p>This has been after c. 200 runs &#40;representing &lt;20&#37; of the available test cases&#41;. I decided to pick these as the new &quot;optimal&quot; parameters and re-run the full benchmark &#40;I did the same for all other models as well&#41;.</p> <p>But here&#39;s the twist—repeating the benchmark revealed no significant change. After a bit of sleuthing, I discovered the API&#39;s model had been updated, rendering my &quot;optimal&quot; parameters outdated.</p> <p>See how the same heatmap looked one day later:</p> <p><img src="/assets/llm_code_generation_experiment/mistral-medium-parameter-search-stage2-20231216.png" alt=mistral-medium-later-results  /></p> <p>&quot;Wait, isn&#39;t it just because you didn&#39;t run enough samples?&quot;</p> <p>While it&#39;s valid to point out the stochastic behavior of these models, with scores potentially fluctuating from one minute to the next, my multi-stage experiment displayed a remarkable consistency in the top-performing parameters &#40;different on each day&#41;. This consistency suggests that, despite the inherent randomness, there seem to be some &#39;optimal&#39; settings that can be identified for specific classes of problems.</p> <p><strong>So, are there optimal parameters?</strong> Yes, but they&#39;re fleeting, tied to the API&#39;s &#43; model&#39;s current version. </p> <p><strong>Is it worth obsessing over them?</strong> For most use cases, probably not. </p> <p><strong>The takeaway?</strong> Focus on a robust evaluation dataset, and let the API handle the rest.</p> <p>Curiosity led to this experiment, and while the pursuit of perfection is alluring, the shifting nature of AI models means that we&#39;re better off embracing adaptability in our everyday use.</p> <p>If you&#39;re interested in the results for all the other models I tested, check out the appendix below.</p> <p>A few observations:</p> <ul> <li><p>You want to think about <code>top_p</code> and <code>temperate</code> together, not in isolation</p> <li><p>Keep their sum around 1.0 &#40;or at least lower than the default 0.7&#43;1.0 &#61; 1.7&#41;</p> </ul> <h1 id=appendix_winning_hyperparameters_for_each_model ><a href="#appendix_winning_hyperparameters_for_each_model" class=header-anchor >Appendix: &quot;Winning&quot; Hyperparameters for each Model</a></h1> <p>Dive into the appendix for a granular view of each model&#39;s performance in our experiments. It&#39;s worth mentioning that I&#39;ve recently enhanced the evaluation parser to more equitably assess smaller OSS models. This adjustment may have caused a slight shift in the results. You might notice a few &quot;high scores&quot; that are supported by a limited number of samples; these are remnants of the previous scoring system and should be interpreted with caution.</p> <p>In other words, don&#39;t take these results as gospel. Instead, use them as a starting point for your own experiments.</p> <h2 id=gpt-4-1106-preview ><a href="#gpt-4-1106-preview" class=header-anchor >GPT-4-1106-Preview</a></h2> <p>The GPT-4-1106-Preview model showed remarkable adaptability in the grid search, with the top three hyperparameter combinations centered around extremes of temperature and top<em>p. Notably, the combination with a low temperature of 0.1 and a high top</em>p of 0.9 yielded the highest score of approximately 87.22. This suggests a preference for highly deterministic output with a wide selection pool, a setting that may be beneficial for generating more creative yet precise code.</p> <p><img src="/assets/llm_code_generation_experiment/gpt-4-1106-preview-parameter-search.png" alt="GPT-4-1106-Preview Heatmap" /></p> <h2 id=mistral-medium ><a href="#mistral-medium" class=header-anchor >Mistral-Medium</a></h2> <p>Mistral-Medium displayed a significant increase in performance when the temperature was set high at 0.9, coupled with a more selective top_p of 0.3, scoring around 82.81. This indicates that a warmer temperature, allowing for more diverse responses, in combination with a moderate selection probability, optimizes performance for this model.</p> <p><img src="/assets/llm_code_generation_experiment/mistral-medium-parameter-search.png" alt="Mistral-Medium Heatmap" /></p> <h2 id=gpt-35-turbo-1106 ><a href="#gpt-35-turbo-1106" class=header-anchor >GPT-3.5-Turbo-1106</a></h2> <p>For GPT-3.5-Turbo-1106, the best results came from a high temperature of 0.9 and a low top_p of 0.1, with a score close to 81.25. This pattern aligns with a tendency towards creative responses but with a narrow choice spectrum, which seems to enhance performance for this particular model.</p> <p><img src="/assets/llm_code_generation_experiment/gpt-3.5-turbo-1106-parameter-search.png" alt="GPT-3.5-Turbo-1106 Heatmap" /></p> <h2 id=mistral-small ><a href="#mistral-small" class=header-anchor >Mistral-Small</a></h2> <p>Note: Due to the evaluation parser improvements, the scores for the mistral-small model have changed slightly. The highest scoring combination with sufficient sample size is still 0.9/0.3 &#40;same as mistral-medium&#41;, the highest value in the heatmap &#40;85.0&#41; does not have sufficient sample size &#40;only 1 run&#41;.</p> <p><img src="/assets/llm_code_generation_experiment/mistral-small-parameter-search.png" alt="Mistral-Small Heatmap" /></p> <h2 id=gpt-35-turbo ><a href="#gpt-35-turbo" class=header-anchor >GPT-3.5-Turbo</a></h2> <p>The GPT-3.5-Turbo favored a temperature of 0.9 and top<em>p set at 0.5 yielding 70.39. However, this score is fairly closed to a more balanced setting of 0.5 for both temperature and top</em>p with medium variability and selection probability, which achieved a score of approximately 68.11.</p> <p><img src="/assets/llm_code_generation_experiment/gpt-3.5-turbo-parameter-search.png" alt="GPT-3.5-Turbo Heatmap" /></p> <h2 id=mistral-tiny ><a href="#mistral-tiny" class=header-anchor >Mistral-Tiny</a></h2> <p>Note: All the re-sampled combinations from Stage 2 drop off this table to performance ~0.5. Ie, no need to keep re-sampling the &quot;top&quot; combinations, they are just a noise/lucky shot.</p> <p><img src="/assets/llm_code_generation_experiment/mistral-tiny-parameter-search.png" alt="Mistral-Tiny Heatmap" /></p> <div class=page-foot > <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> Jan Siml. Last modified: December 10, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia asdasdas programming language</a>. </div> </div></div> <footer class="container mx-auto px-4 py-4 border-t"> <p class="text-right mb-3"> <a href="#" class="text-gray-600 hover:text-gray-900 transition-colors">Back to top</a> </p> <div class="flex flex-col items-center justify-center space-y-3"> <div class=footer-icons > <ul class="flex items-center space-x-4"> <li class="text-gray-700 font-medium">Follow: <li> <a href="https://github.com/svilupp" class="flex items-center space-x-2 text-gray-600 hover:text-gray-900 transition-colors" rel="nofollow noopener noreferrer"> <svg xmlns="http://www.w3.org/2000/svg" width=20  height=20  fill=currentColor  class="bi bi-github" viewBox="0 0 16 16"> <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z"/> </svg> <span>GitHub</span> </a> </ul> </div> <p class="text-sm text-gray-600"> &copy; 2024 · <a href="#" class="hover:text-gray-900 transition-colors">Privacy</a> · <a href="#" class="hover:text-gray-900 transition-colors">Terms</a> </p> </div> </footer> <script src="/libs/highlight/highlight.min.js"></script> <script>hljs.highlightAll();hljs.configure({tabReplace: ' '});</script> <script src="https://cdn.jsdelivr.net/npm/@docsearch/js@3"></script> <script> // Mobile menu toggle const mobileMenuButton = document.querySelector('[aria-controls="mobile-menu"]'); const mobileMenu = document.getElementById('mobile-menu'); mobileMenuButton.addEventListener('click', () => { const expanded = mobileMenuButton.getAttribute('aria-expanded') === 'true'; mobileMenuButton.setAttribute('aria-expanded', !expanded); mobileMenu.classList.toggle('hidden'); }); </script>