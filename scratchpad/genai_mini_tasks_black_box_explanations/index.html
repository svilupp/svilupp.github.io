<!doctype html> <html lang=en > <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start': new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0], j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src= 'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f); })(window,document,'script','dataLayer','GTM-M28VNQP');</script> <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/libs/highlight/styles/github.min.css"> <link href="/css/franklin.css" rel=stylesheet > <link href="/css/vela.css" rel=stylesheet > <script src="/libs/vela/jquery.min.js"></script> <link rel=icon  href="/assets/favicon.png"> <title>Jan's Scratchpad</title> <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-M28VNQP" height=0  width=0  style="display:none;visibility:hidden"></iframe></noscript> <div class="main-nav slideout-menu slideout-menu-left" id=menu > <div class=flex-container > <span class=sidebar-brand > <h3 style='font-size: 25px'>Scratchpad</h3> </span> </div> <nav class=sidebar-nav > <ul class=metismenu  id=metismenu  > <li><a href="/index.html">Home</a> <li><a href="/scratchpad/">Posts</a> <li><a href="/about/">About</a> <li><a href="/tag/">Tags</a> <li><a href="/privacy_policy/">Privacy Policy</a> <li><a href="/cookie_policy/">Cookie Policy</a> </ul> </nav> </div> <main id=panel  class="slidout-panel slideout-panel-left"> <div class="toggle-button hamburger hamburger--spin"> <div class=hamburger-box > <div class=hamburger-inner ></div> </div> </div> <h1 class="page title">Beyond Black Box: Enhancing Model Explainability with LLMs and SHAP</h1> <hr> <div class=franklin-content ><h1 id=tldr ><a href="#tldr" class=header-anchor >TL;DR</a></h1> <p>This tutorial showcases how to use GenAI tools to convert complex SHAP values from a machine learning model into simple, narrative explanations, making model predictions easily understandable and communicable to non-technical stakeholders.</p> <p><div class=franklin-toc ><ol><li><a href="#introduction">Introduction</a><li><a href="#setting_up_the_experiment">Setting Up the Experiment</a><li><a href="#bridging_the_gap_with_llms">Bridging the Gap with LLMs</a><li><a href="#practical_considerations">Practical Considerations</a><li><a href="#conclusion">Conclusion</a><li><a href="#further_reading_and_resources">Further Reading and Resources</a></ol></div> </p> <h2 id=introduction ><a href="#introduction" class=header-anchor >Introduction</a></h2> <p>In the realm of machine learning, &#39;black box&#39; models often offer impressive predictive performance but lack transparency in their decision-making processes. SHAP &#40;SHapley Additive exPlanations&#41; has emerged as a powerful tool to demystify these models by quantifying the impact of each feature on the prediction. </p> <p>While SHAP values offer a powerful way to understand machine learning predictions, their technical nature often leaves stakeholders in the dark. This tutorial introduces a swift, GenAI-driven solution that transforms these intricate explanations into relatable narratives, making complex model insights easily shareable and understandable for all involved parties.</p> <p>This blog post explores how Large Language Models &#40;LLMs&#41; can transform your predictions into intuitive, narrative explanations, using the Titanic dataset as a case study. We&#39;ll leverage the Julia programming language, specifically the MLJ.jl framework for machine learning and SHAP.jl for generating SHAP values.</p> <p>It is inspired by the paper <a href="https://arxiv.org/pdf/2309.17057v1.pdf">Tell Me a Story&#33; Narrative-Driven XAI with Large Language Models</a>, where the authors share interesting findings from a user study:</p> <blockquote> <p>...over 90&#37; of the surveyed general audience finds the narrative generated by SHAPstories convincing.</p> <p>excerpt from <a href="https://arxiv.org/pdf/2309.17057v1.pdf">Tell Me a Story&#33; Narrative-Driven XAI with Large Language Models</a></p> </blockquote> <h2 id=setting_up_the_experiment ><a href="#setting_up_the_experiment" class=header-anchor >Setting Up the Experiment</a></h2> <p>The Titanic dataset, a classic in machine learning, offers a vivid context for survival prediction. Since it&#39;s not the focus of this article, we&#39;ll borrow the approach from the <a href="https://forem.julialang.org/mlj/julia-boards-the-titanic-1ne8">Titanic MLJ tutorial</a>.</p> <pre><code class="julia hljs"><span class=hljs-comment ># To set up your environment, you&#x27;ll need to install the following packages:</span>
<span class=hljs-comment ># ]add DataFramesMeta MLJ DecisionTree MLJDecisionTreeInterface ShapML PromptingTools </span>

<span class=hljs-keyword >using</span> MLJ, DataFramesMeta, ShapML
<span class=hljs-keyword >using</span> MLJ.CategoricalArrays: unwrap
<span class=hljs-keyword >using</span> PromptingTools
<span class=hljs-keyword >const</span> PT = PromptingTools

<span class=hljs-comment ># Get the data and transform it into MLJ format</span>
table = OpenML.load(<span class=hljs-number >42638</span>)
df = DataFrame(table)
dropmissing!(df, :embarked)
coerce!(df, :sibsp =&gt; Count, :survived =&gt; OrderedFactor, :pclass =&gt; OrderedFactor, :sex =&gt; OrderedFactor, :embarked =&gt; OrderedFactor)
y, X = unpack(df, ==(:survived), !=(:cabin));
train, test = partition(eachindex(y), <span class=hljs-number >0.7</span>, shuffle=<span class=hljs-literal >true</span>, rng=<span class=hljs-number >1234</span>);

<span class=hljs-comment ># Train a model on 70% of data</span>
Tree = <span class=hljs-meta >@load</span> DecisionTreeClassifier pkg = DecisionTree
tree = Tree(max_depth=<span class=hljs-number >5</span>)
mach = machine(tree, X, y)
fit!(mach, rows=train)
y_pred = predict(mach, X);</code></pre> <p>Next, we integrate SHAP.jl to compute SHAP values for our model&#39;s predictions:</p> <pre><code class="julia hljs"><span class=hljs-comment ># Generate SHAP values for the classifier predictions</span>
predict_proba(model, data) = DataFrame(; y_pred=predict(model, data) |&gt; x -&gt; pdf.(x, <span class=hljs-string >&quot;1&quot;</span>))
data_shap = ShapML.shap(explain=X,
    model=mach,
    predict_function=predict_proba,
    sample_size=<span class=hljs-number >60</span>,
    seed=<span class=hljs-number >1234</span>
);</code></pre> <p>Let&#39;s show the SHAP results of a single data point:</p> <pre><code class="julia hljs">shap_ = <span class=hljs-meta >@chain</span> <span class=hljs-keyword >begin</span>
    <span class=hljs-meta >@rsubset</span> data_shap :index == <span class=hljs-number >3</span>
    <span class=hljs-meta >@orderby</span> -:shap_effect
<span class=hljs-keyword >end</span></code></pre> <pre><code class="plaintext hljs">6×6 DataFrame
 Row │ index  feature_name  feature_value  shap_effect  shap_effect_sd  intercept 
     │ Int64  String        Any            Float64      Float64         Float64   
─────┼────────────────────────────────────────────────────────────────────────────
   1 │     3  sex           female          0.349978         0.288405    0.350693
   2 │     3  sibsp         0               0.0166667        0.129099    0.350693
   3 │     3  fare          7.925           0.0165051        0.177344    0.350693
   4 │     3  embarked      S              -0.00239717       0.0130181   0.350693
   5 │     3  age           26.0           -0.0193997        0.237259    0.350693
   6 │     3  pclass        3              -0.125878         0.247836    0.350693</code></pre> <p>Confusing, right? Let&#39;s try to make sense of these values. </p> <p>&#40;See the <a href="https://nredell.github.io/ShapML.jl/dev/">SHAP.jl documentation</a> for more details on SHAP effect.&#41;</p> <h2 id=bridging_the_gap_with_llms ><a href="#bridging_the_gap_with_llms" class=header-anchor >Bridging the Gap with LLMs</a></h2> <p>Here we introduce the use of LLMs to create stories that explain predictions in a more human-friendly manner. </p> <p>We can create a story template &#40;already done in PromptingTools.jl under the name <code>StorytellerExplainSHAP</code>&#41; and use <code>aigenerate</code> to interpolate these values into a coherent narrative.</p> <p>Let&#39;s prepare the general information about the task and dataset:</p> <pre><code class="julia hljs"><span class=hljs-comment ># Describe the data, perhaps columns names could suffice?</span>
feature_description = <span class=hljs-keyword >let</span>
    io = <span class=hljs-built_in >IOBuffer</span>()
    show(io, describe(X, :mean, :min, :max, :nunique); summary=<span class=hljs-literal >false</span>, eltypes=<span class=hljs-literal >false</span>)
    <span class=hljs-string >&quot;\n&quot;</span> * <span class=hljs-built_in >String</span>(take!(io))
<span class=hljs-keyword >end</span>

<span class=hljs-comment ># to provide to aigenerate as kwargs</span>
task_context = (; task_definition=<span class=hljs-string >&quot;which of the Titanic passenger have died or survived based on their data&quot;</span>,
    feature_description, label_definition=<span class=hljs-string >&quot;that the passenger survived&quot;</span>,
    <span class=hljs-comment ># keep instructions None for now, see `Practical Considerations` section below</span>
    instructions=<span class=hljs-string >&quot;None.&quot;</span>);</code></pre> <p>Let&#39;s prepare a utility function for individual instances &#40;<code>idx</code> is the position of the instance in the dataset&#41;:</p> <pre><code class="julia hljs"><span class=hljs-string >&quot;Prepares the context for the selected instance to be provided to the LLM&quot;</span>
<span class=hljs-keyword >function</span> prepare_instance_context(data_shap, y_pred, y, idx::<span class=hljs-built_in >Int</span>)
    proba_ = pdf(y_pred[idx], <span class=hljs-string >&quot;1&quot;</span>)
    shap_ = data_shap.intercept[<span class=hljs-number >1</span>] + (<span class=hljs-meta >@rsubset</span>(data_shap, :index == idx).shap_effect |&gt; sum)

    probability_pct = proba_ &gt;= <span class=hljs-number >0.5</span> ? round(<span class=hljs-built_in >Int</span>, proba_ * <span class=hljs-number >100</span>) : round(<span class=hljs-built_in >Int</span>, (<span class=hljs-number >1</span> - proba_) * <span class=hljs-number >100</span>)
    prediction = proba_ &gt;= <span class=hljs-number >0.5</span> ? <span class=hljs-string >&quot;the passenger survived&quot;</span> : <span class=hljs-string >&quot;the passenger died&quot;</span>
    outcome = unwrap(y[idx] == <span class=hljs-string >&quot;1&quot;</span>) ? <span class=hljs-string >&quot;the passenger survived&quot;</span> : <span class=hljs-string >&quot;the passenger died&quot;</span>
    classified_correctly = prediction == outcome ? <span class=hljs-string >&quot;correctly classified&quot;</span> : <span class=hljs-string >&quot;misclassified&quot;</span>

    <span class=hljs-meta >@info</span> <span class=hljs-string >&quot;Selected item: <span class=hljs-variable >$idx</span>, Proba: <span class=hljs-variable >$proba_</span> vs SHAP values for instance <span class=hljs-variable >$shap_</span> -&gt; Outcome: <span class=hljs-variable >$outcome</span>&quot;</span>

    <span class=hljs-comment ># Generate the SHAP table</span>
    io = <span class=hljs-built_in >IOBuffer</span>()
    shap_ = <span class=hljs-meta >@chain</span> <span class=hljs-keyword >begin</span>
        <span class=hljs-meta >@rsubset</span> data_shap :index == idx
        <span class=hljs-meta >@rsubset</span> !(:shap_effect ≈ <span class=hljs-number >0</span>)
        <span class=hljs-meta >@orderby</span> -:shap_effect
        <span class=hljs-meta >@rtransform</span> :shap_effect = round(:shap_effect, digits=<span class=hljs-number >2</span>)
        <span class=hljs-meta >@select</span> :feature_name :feature_value :shap_effect
    <span class=hljs-keyword >end</span>
    show(io, shap_; summary=<span class=hljs-literal >false</span>, eltypes=<span class=hljs-literal >false</span>)
    shap_table = <span class=hljs-built_in >String</span>(take!(io))

    <span class=hljs-keyword >return</span> (; probability_pct, prediction, outcome, classified_correctly, shap_table)
<span class=hljs-keyword >end</span>;</code></pre> <p>We&#39;re ready to start generating stories&#33;</p> <p><strong>Example 1: Misclassified</strong></p> <pre><code class="julia hljs">instance_context = prepare_instance_context(data_shap, y_pred, y, <span class=hljs-number >821</span>)
<span class=hljs-comment >#[ Info: Selected item: 821, Proba: 0.07623318385650224 vs SHAP values for instance 0.12002796217871464 -&gt; Outcome: the passenger survived</span>

msg = aigenerate(:StorytellerExplainSHAP; task_context..., instance_context..., model=<span class=hljs-string >&quot;gpt4t&quot;</span>)</code></pre> <pre><code class="plaintext hljs">[ Info: Tokens: 774 @ Cost: \$0.0122 in 18.0 seconds
AIMessage(&quot;In the grand tapestry of the Titanic&#x27;s voyage, the waves of fate seemed to conspire against a young man, a third-class passenger with little more than the humble fare that booked his passage. He was en route to a new life, with no siblings or spouse along for the journey, a solitary figure amongst the throng. His ticket, priced at a modest sum, suggested a man of simple means, likely overlooked amidst the wealth and splendor of the Titanic&#x27;s more affluent passengers. The ship&#x27;s records indicate that he embarked at Southampton, a bustling port where many souls boarded, unaware of the tragedy that lay ahead. Despite these factors that seemingly stacked the odds against his survival, the young man found a way to defy the cold arithmetic of survival on that fateful night.

The prediction cast a dark shadow, suggesting the young man had perished in the disaster, overwhelmed by the unfortunate confluence of his situation. However, his resolve or perhaps serendipity led him to survive, a living testament to the unpredictability of life and the limitations of even the most finely tuned models of artificial conjecture.&quot;)</code></pre> <p><strong>Example 2: Correctly classified as survived</strong></p> <pre><code class="julia hljs">instance_context = prepare_instance_context(data_shap, y_pred, y, <span class=hljs-number >864</span>)
<span class=hljs-comment ># [ Info: Selected item: 821, Proba: 0.07623318385650224 vs SHAP values for instance 0.12002796217871464 -&gt; Outcome: the passenger survived</span>
msg = aigenerate(:StorytellerExplainSHAP; task_context..., instance_context..., model=<span class=hljs-string >&quot;gpt4t&quot;</span>)</code></pre> <pre><code class="plaintext hljs">[ Info: Tokens: 793 @ Cost: \$0.0128 in 23.1 seconds
AIMessage(&quot;On a fateful night in 1912, aboard the ill-fated Titanic, a middle-aged woman made her journey across the Atlantic. Not hailing from the opulence of first class but also not confined to the cramped conditions of the third class, she traveled in second class comfort, which, on this particular voyage, turned out to be a relatively safer berth. Alone without siblings or spouse, she could decisively move and respond to the ensuing chaos that frigid night. Her gender played a pivotal role as women were prioritized during the lifeboat evacuations. The relatively modest fare she paid for her passage, and her age, while painting a picture of an average middle-class woman, surprisingly did not significantly alter her survival chances.

In the end, the combination of her traveling in second class, her companionship status and the fact that she was a woman in her early 40s, tilted the scales in favor of her survival. Despite the tragedy, she was one of the fortunate to secure a place in a lifeboat and live to tell the tale. The model concluded that this passenger survived, mirroring her real-life outcome, driven mainly by her travel class and gender amidst the Titanic&#x27;s tragic demise.&quot;)</code></pre> <p><strong>Example 3: Correctly classified as died</strong></p> <pre><code class="julia hljs">instance_context = prepare_instance_context(data_shap, y_pred, y, <span class=hljs-number >126</span>)
<span class=hljs-comment ># [ Info: Selected item: 126, Proba: 0.07623318385650224 vs SHAP values for instance 0.12002796217871464 -&gt; Outcome: the passenger died</span>

msg = aigenerate(:StorytellerExplainSHAP; task_context..., instance_context..., model=<span class=hljs-string >&quot;gpt4t&quot;</span>)</code></pre> <pre><code class="plaintext hljs">[ Info: Tokens: 800 @ Cost: \$0.013 in 16.6 seconds
AIMessage(&quot;On a fateful night aboard the Titanic, amidst the cold embrace of the Atlantic, the destiny of a young man was sealed. This traveler, a male aged 30, had boarded the vessel with a third-class ticket, a choice that offered limited access to lifeboats and safety measures as the tragedy unfolded around him. His journey had commenced at Queenstown, now known as Cobh, a port indicated by a single-letter ticket stamp &#x27;Q&#x27;, marking his point of embarkation, a detail as undistinguished as the modest fare he had paid for his passage, a mere 7.75 dollars.

In these desperate times, when families clung together and women and children were ushered to safety first, his solitary status, with not a sibling or spouse to claim companionship, rendered him nearly invisible in the chaos. The narrative ends with the echo of his footsteps fading into silence, the algorithm&#x27;s analysis as cold and unyielding as the night, concluding his fate as one among the lost souls of the Titanic. The model, with solemn certainty, predicts his demise, a story woven from the threads of his social standing, his gender, and the austerity of his journey—a young man, alone and in third class, claimed by the sea.&quot;)</code></pre> <p>While the stories are not perfect, they are surprisingly easy to understand. Another GenAI success with just a few lines of extra code&#33;</p> <h2 id=practical_considerations ><a href="#practical_considerations" class=header-anchor >Practical Considerations</a></h2> <p><strong>Choosing the Right Model</strong> I have found only GPT-4 to be useful for this approach. Weaker models would require some finetuning or few-shot prompting to get the desired results.</p> <p>Even though it takes almost 20-30 seconds per story, the overall time is quite reasonable when you send multiple requests in parallel &#40;eg, wih <code>asyncmap</code>&#41;.</p> <p><strong>Costs?</strong> While 1 cent per story might seem like a lot, imagine how long it takes to write functions to produce coherent narratives&#33; You&#39;re saving a lot of time and effort by using this approach.</p> <p>Moreover, you rarely need to explain ALL predictions. It&#39;s usually enough to explain a few examples to get a sense of the model&#39;s behavior or if there is an audit / a request from stakeholders.</p> <p><strong>Boardroom Readiness?</strong> The stories above are not really suitable for a boardroom presentation. However, you can easily provide additional information about the audience, context, style, tone, etc. via <code>instructions</code> placeholder.</p> <pre><code class="julia hljs">msg = aigenerate(:StorytellerExplainSHAP; task_context..., instance_context..., instructions=<span class=hljs-string >&quot;Be brief. Adjust the story for boardroom presentation.&quot;</span>, model=<span class=hljs-string >&quot;gpt4t&quot;</span>)</code></pre>
<pre><code class="plaintext hljs">[ Info: Tokens: 766 @ Cost: \$0.0111 in 19.2 seconds
AIMessage(&quot;In our analysis, we encountered a male passenger aged 27 who had embarked from Southampton without any siblings or spouse aboard. Traveling third class and having paid a fare far below average, he was enveloped by the perilous reputation of the most economically restrained accommodations of the Titanic. This narrative, coupled with being a young male—the archetype often expected to give precedence to women and children during life-saving procedures—cast a long shadow over his likelihood of survival. Our prediction model rendered a grim forecast, plunging the odds against his survival amidst the tragedy well-known to history.&quot;)</code></pre>
<p><strong>What is being sent to the LLM?</strong> If you want to see what we&#39;re sending to the LLM &#40;for debugging&#41;, simply render the template without sending it to the LLM:</p>
<pre><code class="julia hljs">conv_rendered = PT.render(PT.NoSchema(), :StorytellerExplainSHAP; task_context..., instance_context...)
println(conv_rendered[<span class=hljs-number >2</span>].content)</code></pre>
<h2 id=conclusion ><a href="#conclusion" class=header-anchor >Conclusion</a></h2>
<p>Combining SHAP explanations with LLM-generated stories offers a novel way to enhance the interpretability of machine learning models. This approach is particularly valuable for data scientists who need to share the explanations with non-technical stakeholders &#40;Responsible AI&#33;&#41;. e encourage our readers to experiment with this methodology in their own models to gain deeper insights and share their experiences and templates with the community.</p>
<p>Note: This approach is not specific to SHAP. You can use it with any other explainability tool, eg, <a href="https://github.com/JuliaTrustworthyAI/CounterfactualExplanations.jl">CounterfactualExplanations.jl</a>. All you need to do is tweak the prompt template slightly - PromptingTools PRs welcome :&#41;</p>
<hr />
<h2 id=further_reading_and_resources ><a href="#further_reading_and_resources" class=header-anchor >Further Reading and Resources</a></h2>
<ul>
<li><p><a href="https://arxiv.org/pdf/2309.17057v1.pdf">Tell Me a Story&#33; Narrative-Driven XAI with Large Language Models</a></p>

<li><p><a href="https://forem.julialang.org/mlj/julia-boards-the-titanic-1ne8">MLJ.jl Titanic Tutorial</a></p>

<li><p><a href="https://nredell.github.io/ShapML.jl/dev/">SHAP.jl</a></p>

</ul>
<div class=page-foot >
    <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> Jan Siml. Last modified: January 27, 2024.
    Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. See the <a href="/privacy_policy/">Privacy Policy</a>
</div>
</div>
  </main> 
  <script src="/libs/vela/metisMenu.min.js"></script>
  <script src="/libs/vela/slideout.min.js"></script>
  
  
    <script src="/libs/highlight/highlight.min.js"></script>
<script>hljs.highlightAll();hljs.configure({tabReplace: '    '});</script>