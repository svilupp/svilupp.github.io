<!doctype html> <html lang=en > <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start': new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0], j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src= 'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f); })(window,document,'script','dataLayer','GTM-M28VNQP');</script> <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link href="/css/franklin.css" rel=stylesheet > <link href="/css/vela.css" rel=stylesheet > <script src="/libs/vela/jquery.min.js"></script> <link rel=icon  href="/assets/favicon.png"> <title>Jan's Scratchpad</title> <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-M28VNQP" height=0  width=0  style="display:none;visibility:hidden"></iframe></noscript> <div class="main-nav slideout-menu slideout-menu-left" id=menu > <div class=flex-container > <span class=sidebar-brand > <h3 style='font-size: 25px'>Scratchpad</h3> </span> </div> <nav class=sidebar-nav > <ul class=metismenu  id=metismenu  > <li><a href="/index.html">Home</a> <li><a href="/scratchpad/">Posts</a> <li><a href="/about/">About</a> <li><a href="/tag/">Tags</a> <li><a href="/privacy_policy/">Privacy Policy</a> <li><a href="/cookie_policy/">Cookie Policy</a> </ul> </nav> </div> <main id=panel  class="slidout-panel slideout-panel-left"> <div class="toggle-button hamburger hamburger--spin"> <div class=hamburger-box > <div class=hamburger-inner ></div> </div> </div> <h1 class="page title">Is there an optimal temperature and top-p for code generation with paid LLM APIs?</h1> <hr> <div class=franklin-content ><h1 id=tldr ><a href="#tldr" class=header-anchor >TL;DR</a></h1> <p>After experimenting with various API parameters for OpenAI and MistralAI, I found that tweaking two settings—temperature and top_p—could boost code generation performance. But these AI models are like the weather in London; they change so often that today&#39;s &quot;perfect&quot; settings might be outdated by tomorrow. So, rather than chase the elusive &#39;perfect&#39; setup, it&#39;s wiser to focus on creating robust tests for your AI&#39;s performance. Keep it simple and let the AI do the heavy lifting&#33;</p> <p><div class=franklin-toc ><ol><li><a href="#gpt-4-1106-preview">GPT-4-1106-Preview</a><li><a href="#mistral-medium">Mistral-Medium</a><li><a href="#gpt-35-turbo-1106">GPT-3.5-Turbo-1106</a><li><a href="#mistral-small">Mistral-Small</a><li><a href="#gpt-35-turbo">GPT-3.5-Turbo</a><li><a href="#mistral-tiny">Mistral-Tiny</a></ol></div> </p> <p>Last week, the buzz was all about MistralAI&#39;s new API launch, featuring the enigmatic &quot;mistral-medium&quot;—a tier that&#39;s not as widely discussed as the hyped &quot;Mixtral 8x7B&quot; &#40;&quot;mistral-small&quot; on Mistral&#39;s La Plateforme&#41;. Curious, I took my Julia code generation benchmarks for a spin and noticed that &quot;mistral-medium&quot; wasn&#39;t significantly outperforming its smaller sibling.</p> <p>Here&#39;s the catch: such results are only relevant to my mini benchmark and may not hold true across other domains. So, I pondered, could this be due to suboptimal hyperparameters? With that in mind, I decided to tinker with temperature and top_p—parameters that essentially control the creativity and focus of the AI&#39;s responses.</p> <p>I needed a test dataset. Fortunately, there is a <a href="https://github.com/svilupp/Julia-LLM-Leaderboard">Julia-LLM-Leaderboard</a> which has a collection of Julia code generation tasks with a corresponding automated evaluation framework. Each run can score between 0-100 points, where 100 points is the best.</p> <p>My experiment was straightforward. I ran a grid search across 36 combinations of temperature and top_p values, refining the process until I found what seemed like &quot;sweet spots.&quot; &#40;<a href="https://github.com/svilupp/Julia-LLM-Leaderboard/tree/main/experiments/hyperparams-search-paid-apis-v01">detail here</a>&#41;. I did the same for 3 OpenAI and 3 Mistral models.</p> <p>Interestingly, mistral-medium&#39;s performance soared from 54 to 87 points by adjusting to top_p: 0.3 and temperature: 0.9. </p> <p><img src="/assets/llm_code_generation_experiment/mistral-medium-parameter-search-stage2-20231215.png" alt=mistral-medium-first-results  /></p> <p>This has been after c. 200 runs &#40;representing &lt;20&#37; of the available test cases&#41;. I decided to pick these as the new &quot;optimal&quot; parameters and re-run the full benchmark &#40;I did the same for all other models as well&#41;.</p> <p>But here&#39;s the twist—repeating the benchmark revealed no significant change. After a bit of sleuthing, I discovered the API&#39;s model had been updated, rendering my &quot;optimal&quot; parameters outdated.</p> <p>See how the same heatmap looked one day later:</p> <p><img src="/assets/llm_code_generation_experiment/mistral-medium-parameter-search-stage2-20231216.png" alt=mistral-medium-later-results  /></p> <p>&quot;Wait, isn&#39;t it just because you didn&#39;t run enough samples?&quot;</p> <p>While it&#39;s valid to point out the stochastic behavior of these models, with scores potentially fluctuating from one minute to the next, my multi-stage experiment displayed a remarkable consistency in the top-performing parameters &#40;different on each day&#41;. This consistency suggests that, despite the inherent randomness, there seem to be some &#39;optimal&#39; settings that can be identified for specific classes of problems.</p> <p><strong>So, are there optimal parameters?</strong> Yes, but they&#39;re fleeting, tied to the API&#39;s &#43; model&#39;s current version. </p> <p><strong>Is it worth obsessing over them?</strong> For most use cases, probably not. </p> <p><strong>The takeaway?</strong> Focus on a robust evaluation dataset, and let the API handle the rest.</p> <p>Curiosity led to this experiment, and while the pursuit of perfection is alluring, the shifting nature of AI models means that we&#39;re better off embracing adaptability in our everyday use.</p> <p>If you&#39;re interested in the results for all the other models I tested, check out the appendix below.</p> <p>A few observations:</p> <ul> <li><p>You want to think about <code>top_p</code> and <code>temperate</code> together, not in isolation</p> <li><p>Keep their sum around 1.0 &#40;or at least lower than the default 0.7&#43;1.0 &#61; 1.7&#41;</p> </ul> <h1 id=appendix_winning_hyperparameters_for_each_model ><a href="#appendix_winning_hyperparameters_for_each_model" class=header-anchor >Appendix: &quot;Winning&quot; Hyperparameters for each Model</a></h1> <p>Dive into the appendix for a granular view of each model&#39;s performance in our experiments. It&#39;s worth mentioning that I&#39;ve recently enhanced the evaluation parser to more equitably assess smaller OSS models. This adjustment may have caused a slight shift in the results. You might notice a few &quot;high scores&quot; that are supported by a limited number of samples; these are remnants of the previous scoring system and should be interpreted with caution.</p> <p>In other words, don&#39;t take these results as gospel. Instead, use them as a starting point for your own experiments.</p> <h2 id=gpt-4-1106-preview ><a href="#gpt-4-1106-preview" class=header-anchor >GPT-4-1106-Preview</a></h2> <p>The GPT-4-1106-Preview model showed remarkable adaptability in the grid search, with the top three hyperparameter combinations centered around extremes of temperature and top<em>p. Notably, the combination with a low temperature of 0.1 and a high top</em>p of 0.9 yielded the highest score of approximately 87.22. This suggests a preference for highly deterministic output with a wide selection pool, a setting that may be beneficial for generating more creative yet precise code.</p> <p><img src="/assets/llm_code_generation_experiment/gpt-4-1106-preview-parameter-search.png" alt="GPT-4-1106-Preview Heatmap" /></p> <h2 id=mistral-medium ><a href="#mistral-medium" class=header-anchor >Mistral-Medium</a></h2> <p>Mistral-Medium displayed a significant increase in performance when the temperature was set high at 0.9, coupled with a more selective top_p of 0.3, scoring around 82.81. This indicates that a warmer temperature, allowing for more diverse responses, in combination with a moderate selection probability, optimizes performance for this model.</p> <p><img src="/assets/llm_code_generation_experiment/mistral-medium-parameter-search.png" alt="Mistral-Medium Heatmap" /></p> <h2 id=gpt-35-turbo-1106 ><a href="#gpt-35-turbo-1106" class=header-anchor >GPT-3.5-Turbo-1106</a></h2> <p>For GPT-3.5-Turbo-1106, the best results came from a high temperature of 0.9 and a low top_p of 0.1, with a score close to 81.25. This pattern aligns with a tendency towards creative responses but with a narrow choice spectrum, which seems to enhance performance for this particular model.</p> <p><img src="/assets/llm_code_generation_experiment/gpt-3.5-turbo-1106-parameter-search.png" alt="GPT-3.5-Turbo-1106 Heatmap" /></p> <h2 id=mistral-small ><a href="#mistral-small" class=header-anchor >Mistral-Small</a></h2> <p>Note: Due to the evaluation parser improvements, the scores for the mistral-small model have changed slightly. The highest scoring combination with sufficient sample size is still 0.9/0.3 &#40;same as mistral-medium&#41;, the highest value in the heatmap &#40;85.0&#41; does not have sufficient sample size &#40;only 1 run&#41;.</p> <p><img src="/assets/llm_code_generation_experiment/mistral-small-parameter-search.png" alt="Mistral-Small Heatmap" /></p> <h2 id=gpt-35-turbo ><a href="#gpt-35-turbo" class=header-anchor >GPT-3.5-Turbo</a></h2> <p>The GPT-3.5-Turbo favored a temperature of 0.9 and top<em>p set at 0.5 yielding 70.39. However, this score is fairly closed to a more balanced setting of 0.5 for both temperature and top</em>p with medium variability and selection probability, which achieved a score of approximately 68.11.</p> <p><img src="/assets/llm_code_generation_experiment/gpt-3.5-turbo-parameter-search.png" alt="GPT-3.5-Turbo Heatmap" /></p> <h2 id=mistral-tiny ><a href="#mistral-tiny" class=header-anchor >Mistral-Tiny</a></h2> <p>Note: All the re-sampled combinations from Stage 2 drop off this table to performance ~0.5. Ie, no need to keep re-sampling the &quot;top&quot; combinations, they are just a noise/lucky shot.</p> <p><img src="/assets/llm_code_generation_experiment/mistral-tiny-parameter-search.png" alt="Mistral-Tiny Heatmap" /></p> <div class=page-foot > <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> Jan Siml. Last modified: March 08, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. See the <a href="/privacy_policy/">Privacy Policy</a> </div> </div> </main> <script src="/libs/vela/metisMenu.min.js"></script> <script src="/libs/vela/slideout.min.js"></script>