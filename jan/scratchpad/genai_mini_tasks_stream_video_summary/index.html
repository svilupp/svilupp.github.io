<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/libs/highlight/styles/github.min.css"> <!-- bootstrap@5.3.1 and bootstrap icon@1.10--> <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/css/bootstrap.min.css" rel=stylesheet  integrity="sha384-4bw+/aepP/YC94hEpVNVgiZdgIC5+VKNBQNGCHeKRQN+PtmoHDEXuppvnDJzQIu9" crossorigin=anonymous > <link rel=stylesheet  href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.5/font/bootstrap-icons.css"> <link rel=stylesheet  href="/css/style.css"> <link rel=stylesheet  href="https://cdn.jsdelivr.net/npm/@docsearch/css@3"> <link data-n-head=ssr  rel=stylesheet  href="https://fonts.googleapis.com/css?family=Roboto:100,300,400,500,700,900|Material+Icons"> <link rel=stylesheet  href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha512-z3gLpd7yknf1YoNbCzqRKc4qyor8gaKU1qmn+CShxbuBusANI9QpRohGBreCFkKxLhei6S9CQXFEbbKuqLg0DA==" crossorigin=anonymous  referrerpolicy=no-referrer  /> <link rel=stylesheet  href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha512-NhSC1YmyruXifcj/KFRWoC561YpHpc5Jtzgvbuzx5VozKpWvQ+4nXhPdFgmx8xqexRcpAglTj9sIBWINXa8x5w==" crossorigin=anonymous  referrerpolicy=no-referrer  /> <!-- favicon generated through https://realfavicongenerator.net/--> <link rel=apple-touch-icon  sizes=180x180  href="/assets/icon/apple-touch-icon.png"> <link rel=icon  type="image/png" sizes=32x32  href="/assets/icon/favicon-32x32.png"> <link rel=icon  type="image/png" sizes=16x16  href="/assets/icon/favicon-16x16.png"> <link rel=manifest  href="/assets/icon/site.webmanifest"> <link rel=mask-icon  href="/assets/icon/safari-pinned-tab.svg" color="#5bbad5"> <meta name=msapplication-TileColor  content="#da532c"> <meta name=theme-color  content="#ffffff"> <link rel=stylesheet  href="/_css/custom.css"> <title>GenAI Mini-Tasks: Oh no, I missed a meeting. What now?</title> <header data-bs-theme=dark > <nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark"> <div class=container > <a href="/" class="navbar-brand d-flex align-items-center"> <svg xmlns="http://www.w3.org/2000/svg" width=16  height=16  fill=currentColor  class="bi bi-journal-text me-2" viewBox="0 0 16 16"> <path d="M5 10.5a.5.5 0 0 1 .5-.5h2a.5.5 0 0 1 0 1h-2a.5.5 0 0 1-.5-.5zm0-2a.5.5 0 0 1 .5-.5h5a.5.5 0 0 1 0 1h-5a.5.5 0 0 1-.5-.5zm0-2a.5.5 0 0 1 .5-.5h5a.5.5 0 0 1 0 1h-5a.5.5 0 0 1-.5-.5zm0-2a.5.5 0 0 1 .5-.5h5a.5.5 0 0 1 0 1h-5a.5.5 0 0 1-.5-.5z"/> <path d="M3 0h10a2 2 0 0 1 2 2v12a2 2 0 0 1-2 2H3a2 2 0 0 1-2-2v-1h1v1a1 1 0 0 0 1 1h10a1 1 0 0 0 1-1V2a1 1 0 0 0-1-1H3a1 1 0 0 0-1 1v1H1V2a2 2 0 0 1 2-2z"/> <path d="M1 5v-.5a.5.5 0 0 1 1 0V5h.5a.5.5 0 0 1 0 1h-2a.5.5 0 0 1 0-1H1zm0 3v-.5a.5.5 0 0 1 1 0V8h.5a.5.5 0 0 1 0 1h-2a.5.5 0 0 1 0-1H1zm0 3v-.5a.5.5 0 0 1 1 0v.5h.5a.5.5 0 0 1 0 1h-2a.5.5 0 0 1 0-1H1z"/> </svg> <strong>siml.earth</strong> </a> <button class=navbar-toggler  type=button  data-bs-toggle=collapse  data-bs-target="#navbarCollapse" aria-controls=navbarCollapse  aria-expanded=false  aria-label="Toggle navigation"> <span class=navbar-toggler-icon ></span> </button> <div class="collapse navbar-collapse" id=navbarCollapse > <ul class="navbar-nav me-auto mb-2 mb-md-0"> <li class="nav-item dropdown"> <a class="nav-link dropdown-toggle fw-bold" href="#" id=navbarDropdown  role=button  data-bs-toggle=dropdown  aria-expanded=false > Sites </a> <ul class=dropdown-menu  aria-labelledby=navbarDropdown > <li><h6 class=dropdown-header >Choose a Site</h6> <li><a class=dropdown-item  href="/jan/">Jan's Site</a> <li><a class=dropdown-item  href="/ann/">Ann's Site</a> </ul> <li class=nav-item ><hr class="dropdown-divider mx-2"> <li class=nav-item > <span class="navbar-text px-2 fw-bold d-flex align-items-center">Jan's:</span> <li class=nav-item > <a class=nav-link  aria-current=page  href="/jan/">Home</a> <li class=nav-item > <a class=nav-link  href="/jan/scratchpad/">Posts</a> <li class=nav-item > <a class=nav-link  href="/jan/wip/">Work in Progress</a> <li class=nav-item > <a class=nav-link  href="/jan/about/">About</a> <li class=nav-item > <a class=nav-link  href="/jan/tags/">Tags</a> <li class=nav-item > <a class=nav-link  href="/jan/privacy_policy/">Privacy Policy</a> <li class=nav-item > <a class=nav-link  href="/jan/cookie_policy/">Cookie Policy</a> </ul> </div> </div> </nav> </header> <div class="container py-3 px-3 mx-auto"> <div class=franklin-content ><h1 id=tldr ><a href="#tldr" class=header-anchor >TL;DR</a></h1> <p>Use GenAI and PromptingTools.jl in Julia to quickly summarize missed meetings or webinars in minutes, saving hours of catch-up time with a concise, AI-generated overview that&#39;s easy and efficient.</p> <div class=franklin-toc ><ol><li><a href="#tldr">TL;DR</a><ol><li><a href="#introduction">Introduction</a><li><a href="#how_to_turn_oops_into_ahh">How to turn &quot;oops&quot; into &quot;ahh&quot;</a><li><a href="#example_a_video_recorded_on_stream">Example: A Video Recorded on Stream</a><li><a href="#but_wait_theres_more">But wait, there&#39;s more&#33;</a><li><a href="#tips_for_longer_meetings">Tips for Longer Meetings</a><li><a href="#how_about_privacy">How about privacy?</a><li><a href="#why_not_simply_use_chatgpt">Why not simply use ChatGPT?</a><li><a href="#conclusion">Conclusion</a></ol></ol></div> <h2 id=introduction ><a href="#introduction" class=header-anchor >Introduction</a></h2> <p>Ever found yourself in a pinch for missing a meeting or a webinar? Maybe it slipped your mind, or you skipped it to keep coding &#40;we don&#39;t judge&#33;&#41;. But now you&#39;re scrambling to catch up without sitting through hours of recordings. Fear not&#33; GenAI and PromptingTools.jl are here to rescue your day.</p> <h2 id=how_to_turn_oops_into_ahh ><a href="#how_to_turn_oops_into_ahh" class=header-anchor >How to turn &quot;oops&quot; into &quot;ahh&quot;</a></h2> <p>Steps:</p> <ol> <li><p><strong>Get the Transcript</strong>: Most meetings and webinars have a downloadable transcript. If not, you can usually get it from Chrome Inspector. </p> <ul> <li><p>With Microsoft Stream, jump to the Chrome Inspector, open the Network tab, filter for &quot;streamContent&quot; and reload the video&#33; You&#39;ll see both the flat text file &#40;VTT&#41; version and the JSON-formatted version</p> <li><p>With Zoom, you can download the transcript directly from the Cloud Recordings tab in your browser</p> <li><p>Save the transcript into a text file on your computer</p> </ul> <li><p><strong>Clean and Chunk</strong>: Trim out the fluff from the script &#40;eg, IDs and markup&#41;. Next, break it into chunks &#40;say, every 35000 characters for a model with a 16K context window&#41;. This makes it more digestible for our AI buddy and it can work on it in parallel.</p> <li><p><strong>Let GenAI Do Its Magic</strong>: Send each chunk to GenAI asynchronously. Add instructions to keep the summary succinct.</p> <li><p><strong>Merge and Marvel</strong>: Once processed, stitch the summaries together. You can either display them directly or beautify them in Markdown format.</p> </ol> <p><strong>And voil√†&#33;</strong> In about a minute, you have a concise, to-the-point summary of your missed meeting. You can now jump to the crucial parts if needed.</p> <h2 id=example_a_video_recorded_on_stream ><a href="#example_a_video_recorded_on_stream" class=header-anchor >Example: A Video Recorded on Stream</a></h2> <p>Let&#39;s use a recording of a lightning talk for JuliaCon 2022 and save the transcript.</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> Markdown <span class=hljs-comment ># for nicer display</span>
<span class=hljs-keyword >using</span> PromptingTools
<span class=hljs-keyword >const</span> PT = PromptingTools

fn = <span class=hljs-string >&quot;stream_mmm.txt&quot;</span>
txt = read(fn) |&gt; <span class=hljs-built_in >String</span>
print(first(txt, <span class=hljs-number >50</span>))</code></pre> <p>Preview:</p> <pre><code class="plaintext hljs">WEBVTT

25649d85-10aa-4aba-87cc
00:00:32.798 --&gt; 00:00:33.008
Well.

25649d85-10aa-4aba-87cc
00:00:33.088 --&gt; 00:00:36.460
Come to this lightning talk
about optimizing marketing

25649d85-10aa-4aba-87cc
00:00:36.460 --&gt; 00:00:36.828
spent.</code></pre> <p>We can see that the transcript is not perfect &#40;eg, breaking the word &quot;Welcome&quot;&#41; and that it contains a lot of useless data &#40;the ID 256...&#41;, but we can still get a lot of useful information from it.</p> <p>Let&#39;s load it again, but this time line-by-line skipping the lines starting with &quot;256&quot; &#40;why pay the tokens for it...&#41; We will replace some abbreviations with <code>PromptingTools.replace_words</code> - this is a great utility if you have a list of sensitive words/names that you want to quickly scrub.</p> <pre><code class="julia hljs">words_to_replace = [<span class=hljs-string >&quot;MMM&quot;</span>] <span class=hljs-comment ># this can be also useful to remove sensitive words like `[&quot;Apple, Inc.&quot;, &quot;Samsung&quot;, &quot;Huawei&quot;] -&gt; &quot;Company&quot;`</span>
replacement = <span class=hljs-string >&quot;Mix Media Modelling&quot;</span>

<span class=hljs-comment ># Notice that we skip all the lines starting with 256...</span>
<span class=hljs-comment ># And then we join the lines together into a single string</span>
txt = [PT.replace_words(line, words_to_replace; replacement) <span class=hljs-keyword >for</span> line <span class=hljs-keyword >in</span> readlines(fn) <span class=hljs-keyword >if</span> !startswith(line, <span class=hljs-string >&quot;256&quot;</span>)] |&gt; x -&gt; join(x, <span class=hljs-string >&quot;\n&quot;</span>)

<span class=hljs-comment ># We use the usual trick &quot;maximum 5 words&quot; to make the summary more zoomed-out</span>
msg = aigenerate(:AnalystChaptersInTranscript; transcript=txt, instructions=<span class=hljs-string >&quot;Maximum 3 Chapters. Each bullet point must be maximum 5 words.&quot;</span>, model=<span class=hljs-string >&quot;gpt4t&quot;</span>);
Markdown.parse(msg.content)</code></pre> <p>Voil√†&#33; Notice that we&#39;ve used the Instructions placeholder to zoom out a bit and get a less wordy summary.</p> <pre><code class="plaintext hljs">[ Info: Tokens: 4505 @ Cost: \$0.0524 in 26.9 seconds
AIMessage(&quot;# Chapter 1: Introduction to Marketing Optimization [00:00:32.798]

- Marketing spend optimization discussed.
- Motivational quote highlights waste.
- Issue: identifying effective ad spend.

## Section 1.1: Challenges in Optimization [00:03:44.138]

- Insufficient and unobservable data problematic.
- Underspecified problems with multiple solutions.
- Bayesian framework used for plausibility.

## Section 1.2: Benefits of Julia [00:04:43.198]

- Julia&#x27;s composability advantageous for modeling.
- Contrasted with Facebook&#x27;s mixed-language Robin package.

# Chapter 2: Understanding Media Mix Modeling [00:03:08.978]

- Media mix modeling quantifies marketing.
- Aims to maximize revenue from spend.
- Beware of vendors overestimating their value.

## Section 2.1: Diminishing Returns and Adstock Effect [00:06:49.948]

- Marginal ROAS and diminishing returns examined.
- Hill curve demonstrates diminishing returns effect.
- Adstock accounts for lagged advertising impact.

# Chapter 3: Implementing Optimization Example [00:05:12.618]

- Local business with three channels presented.
- Goal: maximize revenue across channels.
- Model fitted to historical revenue.

## Section 3.1: Analyzing Marketing Contributions [00:07:37.838]

- Revenue contribution by channel measured.
- Marginal ROAS quantifies spending efficiency.
- Disparity in spend versus effect opportunity.

## Section 3.2: Optimal Budget Allocation [00:09:09.038]

- Adjusts marketing spend for optimization.
- Projected benefits through budget reallocation analyzed.
- Bayesian framework contextualizes uncertainty in uplift.
- Suggests experimenting with optimized budget plan.&quot;)</code></pre> <h2 id=but_wait_theres_more ><a href="#but_wait_theres_more" class=header-anchor >But wait, there&#39;s more&#33;</a></h2> <p>What if we wanted to use this approach with an open-source model that has only a 4K context window? Let&#39;s mimic it with the default model GPT-3 Turbo &#40;the older version, not the latest 1106-preview&#41;:</p> <pre><code class="julia hljs">msg = aigenerate(:AnalystChaptersInTranscript; transcript=txt, instructions=<span class=hljs-string >&quot;Maximum 3 Chapters. Each bullet point must be maximum 5 words.&quot;</span>)</code></pre>
<p>We get a familiar error saying that the document is too large for the model context window:</p>
<pre><code class="plaintext hljs">{
  &quot;error&quot;: {
    &quot;message&quot;: &quot;This model&#x27;s maximum context length is 4097 tokens. However, your messages resulted in 7661 tokens. Please reduce the length of the messages.&quot;,
    &quot;type&quot;: &quot;invalid_request_error&quot;,
    &quot;param&quot;: &quot;messages&quot;,
    &quot;code&quot;: &quot;context_length_exceeded&quot;
  }
}</code></pre>
<p>Let&#39;s use our chunking utility <code>PromptingTools.split_by_length</code>, which does what it says on the tin - it splits the text by spaces and ensures that each &quot;chunk&quot; is fewer than <code>max_length</code> characters. I tend to use a rule of thumb of 2,500 characters for each 1K tokens of context &#40;to account for the prompt and leave some space for the response&#41;. </p>
<p>Let&#39;s chunk our text into two parts by splitting on <code>max_length&#61;10_000</code> characters &#40;for 4K tokens&#41;.</p>
<pre><code class="julia hljs">chunked_text = PT.split_by_length(txt; max_length=<span class=hljs-number >10_000</span>)
<span class=hljs-comment ># Output: 2-element Vector{String}: ...</span></code></pre>
<p>Great, we can use that directly in our list comprehension to send each chunk for analysis asynchronously &#40;I don&#39;t like waiting&#41;:</p>
<pre><code class="julia hljs">instructions = <span class=hljs-string >&quot;Maximum 1-2 Chapters. Maximum 2 bullets per Chapter/Section. Each bullet point must be maximum 5 words.&quot;</span>
tasks = [Threads.<span class=hljs-meta >@spawn</span> aigenerate(:AnalystChaptersInTranscript; transcript=chunk, instructions, model=<span class=hljs-string >&quot;gpt3t&quot;</span>) <span class=hljs-keyword >for</span> chunk <span class=hljs-keyword >in</span> PT.split_by_length(txt; max_length=<span class=hljs-number >10_000</span>)]

<span class=hljs-comment ># Output 2-element Vector{Task}:</span>
<span class=hljs-comment >#  Task (runnable) @0x000000014abe6270</span>
<span class=hljs-comment >#  Task (runnable) @0x000000014abe6400</span></code></pre>
<p>A few seconds later, we get the familiar INFO logs announcing that the results are ready:</p>
<pre><code class="plaintext hljs">[ Info: Tokens: 5087 @ Cost: \$0.0052 in 4.5 seconds
[ Info: Tokens: 3238 @ Cost: \$0.0034 in 6.0 seconds</code></pre>
<p>If you want to check if the tasks are done &#40;ie, we received all responses&#41;, you can simply run <code>all&#40;istaskdone, tasks&#41;</code>. If you send a lot of chunks, you might want to disable the INFO logs with <code>verbose&#61;false</code>.</p>
<p>Unfortunately, now we have 2 tasks that have messages in them.  We want to: </p>
<ol>
<li><p>convert tasks to messages with <code>fetch&#40;task&#41;</code></p>

<li><p>extract the content with <code>msg.content</code> and </p>

<li><p>concatenate the messages into a single piece of text</p>

</ol>
<p>We can do it all as a one-liner with <code>mapreduce</code> &#40;it executes the first function argument on each task and then joins them together with the second function argument&#41;:</p>
<pre><code class="julia hljs">mapreduce(x -&gt; fetch(x).content * <span class=hljs-string >&quot;\n&quot;</span>, *, msgs) |&gt; Markdown.parse</code></pre>
<pre><code class="plaintext hljs">Chapter 1: Optimizing Marketing [00:00:32 - 00:07:22]
  ‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°

  Section 1.1: Introduction and Motivation
  ========================================

    ‚Ä¢  Lightning talk about marketing optimization.

    ‚Ä¢  Discusses the challenge of tracking advertising spending effectiveness.

  Section 1.2: Marketing Optimization Strategies
  ==============================================

    ‚Ä¢  Media mix modeling for quantifying marketing benefits.

    ‚Ä¢  Challenges include insufficient data and underspecified problems.

  Chapter 1: Model Fitting and Revenue Impact [00:07:22 - 00:09:12]
  ‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°‚â°

  Section 1.1: Model Fitting Challenges [00:07:22 - 00:08:02]
  ===========================================================

    ‚Ä¢  Different parameters can lead to the same curves.

    ‚Ä¢  Fitting these models is challenging.

  Section 1.2: Revenue Impact Analysis [00:08:04 - 00:09:12]
  ==========================================================

    ‚Ä¢  Search ads contribute almost 10% to revenues.

    ‚Ä¢  Optimizing search ads can yield 4X revenues.</code></pre>
<p>Perfect&#33; It took a minute, cost less than a cent and we have our meeting summary&#33; Note that the Chapter numbering is misaligned as we produced each chunk separately, but that&#39;s not a big deal for our use case of scanning what we&#39;ve missed.</p>
<p>If you want to copy the resulting summary into your text editor, you can replace <code>|&gt; Markdown.parse</code> with <code>|&gt; clipboard</code>&#33;</p>
<h2 id=tips_for_longer_meetings ><a href="#tips_for_longer_meetings" class=header-anchor >Tips for Longer Meetings</a></h2>
<p>For longer meetings &#40;&gt;30 minutes&#41;, I would recommend always chunking your transcript even if your AI model supports large context &#40;&gt;100K tokens&#41;. It is a well-known fact that even GPT-4 Turbo and Claude 2 struggle to utilize the full context length effectively and you might miss some important parts of your meetings.</p>
<p>As a bonus, if you split your transcript into several chunks, they can be analyzed in parallel, which means you&#39;ll get your answers faster&#33;</p>
<h2 id=how_about_privacy ><a href="#how_about_privacy" class=header-anchor >How about privacy?</a></h2>
<p>Handling a sensitive meeting? Switch to Ollama models for enhanced privacy &#40;see previous posts&#41;. Plus, you can always scrub all key entities/names before uploading using our <code>replace_words</code> utility.</p>
<h2 id=why_not_simply_use_chatgpt ><a href="#why_not_simply_use_chatgpt" class=header-anchor >Why not simply use ChatGPT?</a></h2>
<p>Of course, use it whenever you can&#33; The benefits of using PromptingTools.jl are:</p>
<ul>
<li><p>The full power of Julia REPL at your disposal &#40;eg, chunk long documents, merge answers, scrub sensitive information&#41;</p>

<li><p>Automate tasks, eg, &quot;Summarize these 20 meetings and save it as a nicely formatted Markdown file or a Quarto document&quot;</p>

<li><p>Leverage intricate templates in PromptingTools.jl and placeholders in them &#40;eg, just provide the transcript and we take care of the rest&#41;</p>

</ul>
<h2 id=conclusion ><a href="#conclusion" class=header-anchor >Conclusion</a></h2>
<p>With all this saved time, maybe catch another episode of your favorite show? Or dive into another Julia project? The choice is yours&#33;</p>
<div class=page-foot >
    <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> Jan Siml. Last modified: November 25, 2024.
    Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
</div>
</div></div> 

<footer class=container >
  <p class=float-end ><a href="#">Back to top</a></p>
  <div class=footer-icons >
      <ul class=social-icons >
          <li><strong>Follow:</strong>
          <li><a href="https://github.com/svilupp" rel="nofollow noopener noreferrer"><svg xmlns="http://www.w3.org/2000/svg" width=16  height=16  fill=currentColor  class="bi bi-github" viewBox="0 0 16 16">
              <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z"/>
            </svg> GitHub</a>
      </ul>
  </div>
  <p class=copyright >&copy; 2023- Company, Inc. ¬∑ <a href="#">Privacy</a> ¬∑ <a href="#">Terms</a></p>
</footer>


<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/js/bootstrap.bundle.min.js" integrity=sha384-HwwvtgBNo3bZJJLYd8oVXjrBZt8cqVSpeBNS5n7C8IVInixGAoxmnlMuBnhbgrkm  crossorigin=anonymous ></script>


  <script src="/libs/highlight/highlight.min.js"></script>
<script>hljs.highlightAll();hljs.configure({tabReplace: '    '});</script>


<script src="https://cdn.jsdelivr.net/npm/@docsearch/js@3"></script>